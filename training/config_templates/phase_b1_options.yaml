# Phase B.1: Hierarchical Options Framework Configuration
# THIS IS AN EXACT COPY OF phase_a2_sac_sharpe.yaml WITH OPTIONS SECTION ADDED
# DO NOT MODIFY ANY PHASE A VALUES - THEY HAVE BEEN CAREFULLY TUNED
#
# Usage:
#   python training/train_sac_with_options.py \
#     --config training/config_templates/phase_b1_options.yaml \
#     --symbol SPY \
#     --base-sac-checkpoint models/phase_a2_sac_sharpe/SPY/sac_continuous_final.zip

# ============================================================================
# PHASE A CONFIGURATION (EXACT COPY FROM phase_a2_sac_sharpe.yaml)
# ============================================================================
experiment:
  name: "phase_b1_options"  # Changed from "phase_a2_sac_sharpe"
  description: "Phase B.1 hierarchical options training with Phase A base config"
  symbols:
    - SPY   # S&P 500 ETF
    - QQQ   # Nasdaq 100 ETF
    - AAPL  # Apple
    - MSFT  # Microsoft
    - NVDA  # NVIDIA
    - AMZN  # Amazon
    - META  # Meta Platforms
    - TSLA  # Tesla
    - JPM   # JP Morgan
    - XOM   # Exxon Mobil

  # Paths
  data_dir: "data/phase3_splits"
  output_dir: "models/phase_b1_options"  # Changed from "models/phase_a2_sac_sharpe"
  log_dir: "logs/phase_b1_options"  # Changed from "logs/phase_a2_sac_sharpe"
  mlflow_uri: "file:./mlruns"

training:
  total_timesteps: 500000  # INCREASED from 120k → 500k (stable optimizer needs time)
  seed: 1989
  log_freq: 1000  # PERFORMANCE: Reduced from 250 to 1000 (4x less frequent logging)
  entropy_window: 672
  metrics_log_freq: 1000  # PERFORMANCE: Further reduce callback load for faster stepping
  eval_freq: 15000
  n_eval_episodes: 60
  save_best_model: true
  save_interval: 100000  # Save every 100k steps
  save_final_model: true
  n_envs: 16  # PERFORMANCE: Reduced from 40 to 16 (optimal for Windows + single GPU; 2.5x less IPC overhead)

evaluation:
  episodes: 60
  continuous_temperature: 1.99
  stochastic_continuous: true

environment:
  symbol: "SPY"
  data_path: "data/phase3_splits/SPY/train.parquet"
  action_mode: "continuous"
  episode_length: 672  # PERFORMANCE: Reduced from 1344 to 672 (28 days; 2x faster episodes, still sufficient for Sharpe)
  lookback_window: 24
  vec_env_type: "subproc"  # PERFORMANCE: SubprocVecEnv for n_envs=8 is acceptable on Windows
  max_hold_hours: 672  # FIX #9: FROM 8 → 500 (agent decides when to exit, not forced!)
  stop_loss: 0.999  # FIX #10: FROM 0.02 → 0.99 (DISABLE auto stop-loss!)
  take_profit: 0.999  # FIX #10: FROM 0.025 → 0.99 (DISABLE auto take-profit!)
  # FIX #3 & #5: Atomic transaction costs + Anti-collapse exploration
  continuous_settings:
    hold_threshold: 0.001  # REDUCED from 0.0025 → 0.001 to prevent action collapse to HOLD
    max_position_pct: 0.45  # INCREASED from 0.40 → 0.45 (wider position sizing range)
    smoothing_window: 1  # FIX #5: FROM 2 → 1 (remove smoothing bias toward extremes)
    transaction_cost: 0.00005  # FIX #3: FROM 0.0005 → 0.00005 (10x reduction - atomic costs!)
    min_trade_value: 5.0
    min_hold_steps: 1  # REDUCED from 2 → 1 (allow faster exits)
  portfolio_config:
    # ===== MULTI-POSITION SUPPORT =====
    max_positions: 3  # Allow up to 3 concurrent positions (was 1)
    allow_multiple_positions_per_symbol: true  # Enable multiple entries for same symbol
    shorting_enabled: true  # Enable short selling capability
    short_margin_requirement: 1.5  # 150% margin for shorts (regulatory standard)
    
    # Position sizing and risk limits
    min_position_value_pct: 0.02  # Allow smaller probe trades once capital is committed
    max_position_size_pct: 0.45  # FIX #11: FROM 0.10 → 0.45 (match continuous max_position_pct!)
    max_position_loss_pct: 0.999  # FIX #10: FROM 0.05 → 0.99 (DISABLE auto position stop-loss!)
    max_portfolio_loss_pct: 0.999  # FIX #10: FROM 0.20 → 0.99 (DISABLE auto portfolio stop-loss!)
  reward_config:
    # --- SIMPLIFIED REWARD FUNCTION TO MITIGATE HOLD COLLAPSE ---
    pnl_weight: 0.95  # Increased PnL weight to ensure it dominates
    diversity_bonus_weight: 0.15  # MODIFIED: Increased from 0.05 to encourage action diversity and break HOLD pattern.
    
    # Drastically reduce transaction cost weight; base cost is already very low
    transaction_cost_weight: 0.001  # Drastically reduced from 0.10 (100x reduction)
    base_transaction_cost_pct: 0.00005

    # DISABLE ALL SECONDARY PENALTIES INITIALLY to get the agent trading
    time_efficiency_weight: 0.0
    drawdown_weight: 0.0
    sizing_weight: 0.0
    action_repeat_penalty_weight: 0.0
    hold_penalty_weight: 0.040  # MODIFIED: Added a tiny penalty to discourage idling and act as a tie-breaker against HOLD.
    intrinsic_action_reward: 0.01  # Small intrinsic reward to encourage action changes
    equity_delta_weight: 0.0

    # PnL Amplification is good, keep as is
    pnl_scale: 0.0001
    reward_clip: 1250.0

    # Keep Sharpe/ROI mechanics disabled initially, remove conflicting logic
    sharpe_weight: 0.0
    sharpe_gate_enabled: false
    roi_multiplier_enabled: false
    roi_scale_factor: 1.0
    roi_neutral_zone: 0.0001  # CRITICAL: Ensure this is indeed small
    roi_negative_scale: 1.0
    roi_positive_scale: 1.0
    
    # Set multipliers to neutral (1.0) or remove if not applicable with disabled ROI
    win_bonus_multiplier: 2.0  # Neutral, let PnL dictate
    loss_penalty_multiplier: 1.0  # Explicitly strengthen loss penalties as per comment's intent. Was 1.0
                                  # Justification: Balance encouraging trades with not tolerating losses.
                                  # The previous comment's intent was to strengthen, so setting to 1.2.

    # Legacy params (keep for compatibility but ensure they are neutral)
    sharpe_neutral_zone: 1.0
    sharpe_negative_scale: 1.0
    sharpe_positive_scale: 1.0
    sharpe_full_penalty_trades: 240
    sharpe_gate_min_self_trades: 240
    sharpe_gate_floor_scale: 0.3
    sharpe_gate_active_scale: 1.0
    roi_full_penalty_trades: 240
    
    # Set to neutral initially to avoid complex interactions
    position_size_small_multiplier: 1.0
    position_size_medium_multiplier: 1.0
    position_size_large_multiplier: 1.0
    full_exit_multiplier: 1.0
    staged_exit_bonus: 1.0
    partial_exit_multiplier: 1.0
    
    time_decay_threshold_hours: 72.0
    time_decay_penalty_per_hour: 0.004

    # --- SIMPLIFIED ANTI-COLLAPSE & ADVANCED AUGMENTATIONS ---
    # Disable aggressive diversity penalties
    diversity_penalty_weight: 0.0  # CRITICAL: Disable this. It's heavily penalizing 90% HOLD.
    diversity_penalty_target: 0.15
    diversity_penalty_window: 10
    
    # Disable trade frequency penalty
    trade_frequency_penalty_weight: 0.0  # CRITICAL: Disable this. It's penalizing any action.
    hold_bonus_weight: 0.0
    
    # Disable complex reward augmentations
    progressive_roi_enabled: false  # CRITICAL: Disable complex ROI scaling
    progressive_roi_thresholds: [0.05, 0.02, 0.0, -0.01, -0.02]
    progressive_roi_multipliers: [3.0, 2.0, 1.5, 0.5, 1.0, 2.0]
    
    # Momentum is a form of reward shaping, keep it simple for now or set to 0.0
    momentum_weight: 0.0  # Disable momentum initially
    momentum_window: 24
    momentum_max_reward: 0.3
    
    # Disable context-dependent scaling
    context_scaling_enabled: false  # CRITICAL: Disable complex context scaling
    overtrading_threshold: 0.48
    overtrading_penalty_scale: 0.5
    patient_trading_threshold: 0.30
    patient_trading_bonus: 1.2
    low_winrate_threshold: 0.48
    low_winrate_penalty_scale: 0.7
    
    # Disable adaptive win multiplier
    adaptive_win_multiplier_enabled: false  # CRITICAL: Disable adaptive win bonus
    adaptive_win_base_multiplier: 1.8
    adaptive_win_max_multiplier: 2.5
    adaptive_win_min_multiplier: 1.2
    action_concentration_threshold: 0.50
    high_entropy_threshold: 2.5
    overtrading_action_threshold: 0.48
    
    # Keep action entropy bonus to encourage diversity for now
    entropy_bonus_weight: 0.20  # Keep this to still encourage policy exploration
    entropy_bonus_target: 2.5
    entropy_bonus_scale: 0.5
    
  # FIX #5: STRONG EXPLORATION TO PREVENT ACTION COLLAPSE
  epsilon_greedy_enabled: true
  epsilon_start: 0.25  # Restore early exploration strength
  epsilon_end: 0.08  # Maintain modest stochasticity throughout training
  epsilon_decay_steps: 400000  # Slower anneal so exploration persists longer

sac:
  base_learning_rate: 0.00025
  warmup_fraction: 0.10  # Increased from 0.05 to 0.10.
                         # Justification: Slower warmup allows more initial exploration with lower LR, preventing premature convergence to 'HOLD'.
  buffer_size: 200000
  learning_starts: 2000
  batch_size: 256  
  tau: 0.005
  gamma: 0.992
  train_freq: 1
  gradient_steps: 2  
  auto_scale_gradient_steps: false

  # --- CRITICAL FIX FOR SAC ENTROPY MISMATCH ---
  ent_coef: 0.05  # Changed from "auto_0.35" to a fixed, low value.
                  # Justification: The observed ent_coef (1.21) is wildly high, indicating policy instability.
                  # Forcing a low, fixed value initially prevents runaway entropy regularization
                  # and allows the agent to focus on maximizing rewards without being overly
                  # penalized for not being random. We want to find *any* profitable action first.
  target_update_interval: 1
  target_entropy: -1.0  # Keep this, as ent_coef is now fixed, this value won't drive its scale directly.
  ent_coef_lower_bound: 0.05  # Changed from 0.15 to match the new fixed ent_coef.
                              # Justification: Ensure the lower bound is consistent with the fixed value.
  lr_cycles: 4
  lr_min_fraction: 0.25
  use_sde: false
  action_noise_sigma: 0.10
  action_noise_theta: 0.15
  action_noise_dt: 0.02
  use_amp: true
  enable_anomaly_detection: false
  policy_kwargs:
    net_arch:
      - 512
      - 512
    log_std_init: -0.9
  compile_policy: true  # PERFORMANCE: Enable torch.compile for ~15-30% speedup on CUDA (requires triton)
  optimizer_kwargs:
    betas: [0.9, 0.9]
    eps: 1.0e-8
    weight_decay: 0.01

icm:
  # TIER 2 FIX 2.1: Extended ICM exploration throughout full training run
  enabled: true
  eta: 0.006
  beta: 0.2
  hidden_dim: 256
  feature_dim: 128
  extrinsic_weight: 0.97
  intrinsic_weight: 0.003
  intrinsic_warmup_weight: 0.003
  intrinsic_warmup_steps: 10000
  intrinsic_final_weight: 0.0005  # SLOW DECAY from 0.001 → 0.0005 (maintain exploration)
  intrinsic_decay_after_steps: 30000  # EXTENDED from 10000 → 30000 (delay decay start)
  intrinsic_decay_duration: 90000  # EXTENDED from 35000 → 90000 (gradual decay)
  intrinsic_decay_type: cosine
  minimum_intrinsic_weight: 0.0
  disable_when_positive_sharpe: true
  disable_sharpe_threshold: -0.5
  resume_sharpe_threshold: -0.01
  disable_reward_std_threshold: 0.02
  icm_lr: 0.00008
  train_freq: 4  # PERFORMANCE: Reduced from 1 to 4 (train ICM every 4 gradient steps instead of every step; 4x speedup)
  warmup_steps: 3000

# ============================================================================
# PHASE B.1: HIERARCHICAL OPTIONS CONFIGURATION
# ============================================================================
options:
  enabled: true                    # Enable hierarchical options framework
  use_amp: false                   # Force float32 precision for options controller
  
  # Architecture
  state_dim: 578                   # Flattened observation dimension (actual from environment)
                                   # This is automatically computed from observation dict:
                                   # technical.flatten(): 24 × 23 = 552 (includes sentiment)
                                   # sl_probs.flatten(): 3
                                   # position.flatten(): 5
                                   # portfolio.flatten(): 8
                                   # regime.flatten(): 10
                                   # Total: 578
  num_options: 6                   # 6 trading strategies (added short option)
  hidden_dim: 512                  # OPTIMIZED: 256 → 512 for medium state space (578 dims)
  dropout: 0.1                     # OPTIMIZED: 0.2 → 0.1 (less regularization for large net)
  
  # Training Hyperparameters
  options_lr: 3e-4                 # OPTIMIZED: 1e-4 → 3e-4 (faster options learning)
  train_freq: 8                    # OPTIMIZED: 4 → 8 (less frequent, reduce overhead)
  warmup_steps: 2000               # OPTIMIZED: 5000 → 2000 (start options training sooner)
  value_loss_weight: 0.5           # OK - balanced policy/value learning
  grad_clip: 1.0                   # OK - prevents gradient explosion
  
  # Replay Buffer
  option_buffer_size: 80000        # OPTIMIZED: 10000 → 80000 (more experience for meta-learning)
  batch_size: 128                  # OPTIMIZED: 64 → 128 (larger batches for stability)
  
  # ========================================================================
  # Option-Specific Configurations
  # ========================================================================
  
  # Option 0: OpenLongOption - Progressive Position Building
  # NOTE: Sentiment is an AMPLIFIER, not a requirement (defaults to 0.5 neutral)
  # Technicals drive entry decision; sentiment scales position size
  open_long:
    min_confidence: 0.6            # Minimum SL probability for entry
    max_steps: 10                  # Maximum steps to build position
    rsi_oversold_threshold: 40.0   # RSI level for oversold bounces
    max_exposure_pct: 0.10         # Maximum position size (10% of equity)
    min_sentiment: 0.50            # Blocks only if STRONGLY bearish (< 0.35)
    sentiment_scale_enabled: true  # Amplifies size: neutral=1.0x, bullish=1.0-1.4x
  
  # Option 1: OpenShortOption - Progressive Short Building (NEW)
  # NOTE: Sentiment is an AMPLIFIER, not a requirement (defaults to 0.5 neutral)
  # Technicals (RSI, MA, death cross) drive entry; sentiment scales position size
  open_short:
    min_confidence: 0.6            # Minimum SL probability for entry
    max_steps: 10                  # Maximum steps to build short position
    rsi_overbought_threshold: 65.0 # RSI level for overbought (short entry)
    max_exposure_pct: 0.10         # Maximum short position size (10% of equity)
    max_sentiment: 0.45            # Blocks only if STRONGLY bullish (> 0.65)
    sentiment_scale_enabled: true  # Amplifies size: neutral=1.0x, bearish=1.0-1.4x
  
  # Option 2: ClosePositionOption - Exit Management
  close_position:
    profit_target: 0.025           # +2.5% profit for staged exit
    stop_loss: -0.015              # -1.5% stop loss
    partial_threshold: 0.012       # +1.2% for partial profit taking
    min_hold_steps: 2              # Minimum holding period (anti-churning)
    sentiment_exit_threshold: 0.35 # Exit if sentiment very bearish
    sentiment_stop_tighten: 0.40   # Tighten stop when sentiment < this
    use_sentiment_trailing: true   # Enable sentiment-based trailing stops
  
  # Option 3: TrendFollowOption - BIDIRECTIONAL Trend Following
  # NOTE: MA crossover (SMA_10 vs SMA_20) drives entry; sentiment amplifies
  # BIDIRECTIONAL: Follows bullish trends (longs) AND bearish trends (shorts)
  # - Bullish trend (+2% divergence) → Build long positions
  # - Bearish trend (-2% divergence) → Build short positions
  # - Weak trend (< ±1% divergence) → Exit positions
  # - Sentiment amplifies size in both directions (1.0x to 1.4x)
  trend_follow:
    momentum_threshold: 0.02       # 2% SMA divergence for trend (both directions)
    max_position_size: 0.12        # Maximum position size (long or short)
    use_sentiment_scaling: true    # Amplifies size: neutral=1.0x, aligned=1.0-1.4x
  
  # Option 4: ScalpOption - Quick Profit Taking
  # NOTE: RSI oversold (< 35) drives entry; sentiment amplifies position size
  scalp:
    profit_target: 0.010           # +1.0% quick profit target
    stop_loss: -0.005              # -0.5% tight stop
    max_steps: 8                   # Maximum 8-hour holding period
    rsi_entry: 35.0                # RSI threshold for entry (PRIMARY)
    position_size: 0.05            # 5% position size baseline
    min_sentiment_entry: 0.45      # Blocks only if STRONGLY bearish (< 0.35)
    sentiment_exit_threshold: 0.40 # Exit if sentiment very bearish
    use_sentiment_sizing: true     # Amplifies size: neutral=1.0x, bullish=1.0-1.3x
  
  # Option 5: WaitOption - Intelligent Observation
  # NOTE: Sentiment only affects early exit on extremes (0.30/0.75)
  wait:
    max_wait_steps: 20             # Maximum observation period
    min_wait_steps: 3              # Minimum observation period
    sentiment_extreme_high: 0.75   # Exit wait on strong bullish sentiment
    sentiment_extreme_low: 0.30    # Exit wait on strong bearish sentiment
    use_sentiment_exit: true       # Enable sentiment-based early exit

