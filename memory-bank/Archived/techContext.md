# Technical Context
**Scaling Down for Local Operation**
    *   Adjusted system components to function efficiently on local machines.
    *   Optimized resource usage to prevent performance issues.

2.  **Alpaca API Utilization**
    *   Chosen for its robust features and paper trading support.
    *   Streamlined development with a single, comprehensive API.

3.  **Focus on Paper Trading**
    *   Emphasized for risk-free testing and strategy development.
    *   Avoids regulatory complexities associated with live trading.

4.  **User Empowerment**
    *   Provided tools for users to manage symbols and settings.
    *   Enhanced user control over the trading experience.

## LLM Handler Architecture & LoRA Integration


## Core Components
*   **Base Model:** `unsloth/deepseek-r1-distill-llama-8b-unsloth-bnb-4bit` (loaded from `models1/`).
*   **LoRA Adapter:** Fine-tuned adapter weights are generated by `training/lora_trainer1.py` and saved to `training/lora_finetuned_output/`Versioning Loading has been implemented to load the latest trained LORA
*   **Loading:** `models/llm_handler.py` (`LLMHandler`) loads the base model and, when `use_lora=True`, explicitly loads the trained adapter from `training/lora_finetuned_output/`. The module loads the latest trained LORA based on the data of the training.
*   **Input:** The handler receives market data including a 9-feature `context_vector` (derived from Open, High, Low, Close, Volume, VWAP, RSI, volume_pct, returns_1h) and a `sentiment_score`. 10 Features in total
*   **Prompting:** A specific prompt format (`Input: Context=..., Sentiment=... -> Output: Signal=`) is used to query the LoRA-enhanced model.
*   **Output Parsing:** The handler expects the model to output a single digit class label (0-4) immediately following the prompt. It parses this digit.
*   **Decision Mapping:** The parsed class label is mapped to a string decision:
    *   0 (Strong Sell), 1 (Sell) -> "SELL"
    *   2 (Hold) -> "HOLD"
    *   3 (Buy), 4 (Strong Buy) -> "BUY"
*   **Integration:** The `core/decision_engine.py` receives this string decision ("BUY", "SELL", "HOLD") from the `LLMHandler`.
*   **Database:** SQLite (`trading_bot.db`) with WAL mode enabled. Managed via SQLAlchemy. Tables: `trade_journal`, `sentiment_analysis`.
*   **Data Loading:**
    *   `core/hist_data_loader.py`: Fetches historical bar data (1Hour) from Alpaca, handles pagination and potential delays, saves to Parquet files (`data/historical/{SYMBOL}/1Hour/data.parquet`).
    *   `core/historical_context.py`: Loads combined historical data from Parquet, calculates indicators (RSI, returns, volume_pct).
    *   `core/news_sentiment.py`: Fetches news from Alpaca, performs sentiment analysis using `cardiffnlp/twitter-roberta-base-sentiment-latest`, calculates average daily sentiment.
    *   `tests/initial_population.py`: Populates `trade_journal` with historical data and attaches averaged sentiment scores to `sentiment_analysis`. Includes DB verification and finalization steps.
*   **Data Preparation (`core/data_preparation.py`):**
    *   Loads historical data and filtered feedback data (excluding fallback sentiment).
    *   Filters feedback records to ensure sufficient future historical data exists for PnL simulation.
    *   Generates a 10-feature context vector (9 scaled technical indicators + 1 sentiment score).
    *   Simulates trades based on entry price/timestamp:
        *   **Window:** 8 hours
        *   **TP:** +5%
        *   **SL:** -2%
    *   Maps simulation outcome to target classes (0-4).
    *   Applies class balancing (downsampling) to create the final training set.
    *   Saves data to `data/training data/training_data_pnl_v1.jsonl`.
*   **Model Handling (`models/llm_handler.py`):**
    *   Loads the base LLM (`models1`).
    *   Dynamically finds and loads the latest trained LoRA adapter from `training/adapter_runs/`.
    *   Builds prompts matching the PnL training format (Context vector -> Signal).
    *   Performs inference to predict signal class (0-4).
    *   Maps predicted class to trading decision (BUY/SELL/HOLD).
**News Sentiment Processing:**
    *   News articles fetched in parallel (`tests/initial_population.py`).
    *   Unique articles analyzed in batches using `datasets` library integration for efficiency.
    *   Raw score [-1, 1] calculated (`positive_score - negative_score`).
    *   Scores averaged per symbol/date.
    *   Final score [0, 1] normalized (`(raw_score + 1) / 2`).
    *   This final score was used in `core/data_preparation.py` (first version) to generate class labels (0-4) based on thresholds.
*   **Training (`training/lora_trainer1.py`, `run_training.py`):**
    *   Uses Unsloth for optimized LoRA fine-tuning.
    *   Configurable hyperparameters (rank, alpha, epochs, lr, etc.).
    *   Handles dataset loading, formatting, tokenization.
    *   Uses Hugging Face `Trainer`.
    *   Includes post-training evaluation (Accuracy, F1).
    *   `run_training.py` wrapper ensures correct venv and argument passing.
*   **Decision Engine (`core/decision_engine.py`):**
    *   Integrates `LLMHandler` to get predictions.
    *   Applies risk management rules (SL, TP, position sizing - *needs update to match 5/-2/8h*) - needs to be verified and implemented
    *   Executes trades via Alpaca API (placeholder/future).
*   **GPU Utilities (`utils/gpu_utils.py`):**
    *   Detects GPU capabilities.
    *   Sets optimal PyTorch flags (TF32).
    *   Configures efficient attention mechanism (Flash Attention 2 / SDPA).
    *   Calculates optimal batch sizes based on VRAM.
    *   Integrated into training, inference, and data processing scripts.

## Key Technologies
*   Python 3.10+
*   PyTorch (with CUDA)
*   Hugging Face Transformers, PEFT, Datasets, Evaluate
*   Unsloth (for LoRA optimization)
*   SQLAlchemy (for DB interaction)
*   Pandas, NumPy, Scikit-learn
*   Alpaca API (for market data, news, trading)
*   SQLite

## Environment
*   OS: Windows 11
*   Hardware: i5-13600k, RTX 5070 Ti 16GB, 96GB RAM
*   Primary Directory: `C:\TradingBotAI`
*   Virtual Environment: `.venv`
