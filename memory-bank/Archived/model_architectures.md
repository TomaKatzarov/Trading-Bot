# Custom Neural Network Model Architectures

**Document Version:** 1.0
**Date:** 2025-05-28
**Author:** Roo (AI Architect)

## 1. Introduction

This document provides an overview of the custom Neural Network (NN) architectures implemented as part of **Task 1.4: Train Custom NN Models & Tune Hyperparameters**. These models are designed for a binary classification task: predicting a "BUY_SIGNAL" (1) or "NO_BUY_SIGNAL" (0) based on a sequence of financial market features.

The implemented architectures are located in [`core/models/nn_architectures.py`](../core/models/nn_architectures.py:1) and are designed to be compatible with the data generated by [`core/data_preparation_nn.py`](../core/data_preparation_nn.py:1).

Key common features across these models include:
*   **Input:**
    *   Feature sequences: `(batch_size, lookback_window, n_features)`
    *   Asset IDs: `(batch_size,)` for learnable embeddings.
*   **Output:** Single sigmoid unit producing a probability `(batch_size, 1)` for binary classification.
*   **Asset ID Embedding:** All sequential models utilize `nn.Embedding` for asset IDs, concatenating the embedding with input features at each time step, as detailed in [`memory-bank/asset_id_embedding_strategy.md`](../memory-bank/asset_id_embedding_strategy.md:1).
*   **Regularization:** Strategic use of Dropout, Layer Normalization, and Batch Normalization as appropriate for each architecture, informed by [`memory-bank/Diagnostic Report and Remediation Plan for LSTMCNN-LSTM Model Underperformance.md`](../memory-bank/Diagnostic%20Report%20and%20Remediation%20Plan%20for%20LSTMCNN-LSTM%20Model%20Underperformance.md:1).

## 2. Implemented Architectures

The following model architectures have been implemented:

### 2.1. MLP (Multi-Layer Perceptron) - Baseline

*   **Purpose:** Serves as a simple baseline to evaluate the effectiveness of more complex sequential models.
*   **Input Processing:** The input sequence `(lookback_window, n_features)` is flattened. Asset ID embeddings are concatenated to this flattened vector.
*   **Core Layers:**
    *   2-3 fully connected hidden layers (e.g., 128, 64, 32 units) with ReLU activation.
    *   Dropout applied after each hidden layer.
*   **Output Layer:** Single neuron with Sigmoid activation.
*   **Key Configuration:** Number of hidden layers, units per layer, dropout rate.

### 2.2. LSTM (Long Short-Term Memory) with Self-Attention

*   **Purpose:** Primary sequential model for capturing temporal dependencies in the feature sequences.
*   **Core Layers:**
    *   Asset ID Embedding layer.
    *   1-2 LSTM layers (e.g., 64-128 hidden units).
    *   Layer Normalization applied to LSTM layers for training stability.
    *   Custom `SelfAttention` layer applied to the output sequence of the LSTM layers. This allows the model to weigh the importance of different time steps.
    *   Dropout applied after LSTM and Attention layers.
*   **Output Layer:** Fully connected layer followed by Sigmoid activation.
*   **Key Configuration:** LSTM hidden units, number of LSTM layers, attention mechanism parameters, dropout rates, asset embedding dimension.

### 2.3. GRU (Gated Recurrent Unit) with Self-Attention

*   **Purpose:** An alternative sequential model to LSTM, often with similar performance but potentially faster training due to fewer parameters.
*   **Core Layers:**
    *   Asset ID Embedding layer.
    *   1-2 GRU layers (e.g., 64-128 hidden units).
    *   Layer Normalization applied to GRU layers.
    *   Custom `SelfAttention` layer applied to the output sequence of the GRU layers.
    *   Dropout applied after GRU and Attention layers.
*   **Output Layer:** Fully connected layer followed by Sigmoid activation.
*   **Key Configuration:** GRU hidden units, number of GRU layers, attention mechanism parameters, dropout rates, asset embedding dimension.

### 2.4. CNN-LSTM Hybrid (Exploratory)

*   **Purpose:** Explores combining 1D Convolutional Neural Networks (CNNs) for local pattern extraction with LSTMs for modeling longer-term temporal dependencies. Includes improvements based on past diagnostic reports.
*   **Core Layers:**
    *   Asset ID Embedding layer.
    *   1-2 1D Convolutional layers (e.g., 32-64 filters, kernel sizes 3-5) with ReLU activation.
    *   Batch Normalization applied after CNN layers.
    *   The output of the CNN layers (feature maps) is then fed into:
    *   1-2 LSTM layers.
    *   Optional: A `SelfAttention` layer can be applied after the LSTM layers.
    *   Dropout applied throughout the network.
*   **Output Layer:** Fully connected layer followed by Sigmoid activation.
*   **Key Configuration:** CNN filter counts, kernel sizes, LSTM hidden units, number of layers for CNN and LSTM, use of attention, dropout rates, asset embedding dimension.

## 3. Model Creation and Usage

A factory function [`create_model()`](../core/models/nn_architectures.py:485) and an information utility [`get_model_info()`](../core/models/nn_architectures.py:500) are provided in [`core/models/nn_architectures.py`](../core/models/nn_architectures.py:1) for easy instantiation and inspection of these models.

Example:
```python
from core.models import create_model, get_model_info

config = {
    'model_type': 'lstm_attention', # or 'mlp', 'gru_attention', 'cnn_lstm'
    'n_features': 17, # Example: 14 TIs + 2 DoW + 1 Sentiment
    'num_assets': 154,
    'lookback_window': 24,
    'asset_embedding_dim': 8,
    'lstm_hidden_units': 64,
    'attention_heads': 4, # Example if using multi-head attention
    'dropout_rate': 0.3
}

model = create_model(config)
print(get_model_info(model, config))
```

This structure allows for flexible experimentation during the training and hyperparameter tuning phases.