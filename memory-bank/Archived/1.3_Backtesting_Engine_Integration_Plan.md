# Implementation Plan: 1.3 Build Backtesting Engine Integration

**Document Version:** 1.0
**Date:** 2025-05-23
**Author:** Flow-Architect (Lead Software Engineer)
**Task ID:** 1.3 (from `memory-bank/progress.md`)

## 1. Overall Objective for Task 1.3

To develop and integrate a robust backtesting capability that allows for the evaluation of supervised Neural Network (NN) models. This involves designing and implementing a strategy class (`SupervisedNNStrategy`) that translates NN model outputs into trade signals, connecting this strategy to the existing backtesting engine, ensuring proper data flow and inference, implementing comprehensive results logging, and validating the entire system with various test cases, including advanced validation metrics.

## 2. Overall Scope for Task 1.3

### In Scope:
*   Design and implementation of the `SupervisedNNStrategy` class.
*   Definition and implementation of trade entry/exit rules based on NN signals.
*   Integration of NN model inference within the backtesting loop.
*   Testing the strategy with sample data and dummy/real models.
*   Integration of backtesting results logging (PnL, trade counts, metrics).
*   Validation of the backtester and strategy with edge cases.
*   Full integration of the `SupervisedNNStrategy` as the baseline for Phase 1.
*   Finalizing and integrating advanced validation techniques for evaluating strategy performance.

### Out of Scope:
*   Development of new NN model architectures or training of NN models (covered in Task 1.4).
*   Significant modifications to the core backtesting engine itself, beyond what's necessary for strategy integration.
*   Development of the Reinforcement Learning (RL) environment or agents (covered in Phase 2).
*   Live trading deployment.

## 3. Overall Prerequisites for Task 1.3

*   **Completion of Task 1.2:** "Develop/Refine Feature Engineering Pipeline (using `core/data_preparation_nn.py`)" is complete, providing defined feature sets and data formats.
*   **Existing Backtesting Engine:** A functional `core/backtesting` engine with a defined API for strategy integration.
*   **NN Model Artifacts:** Availability of (or ability to create dummy) trained NN models and scalers for testing purposes.
*   **Relevant Documentation:** Access to `memory-bank/feature_set_NN.md`, `memory-bank/strategic_plan_nn_rl.md`, `memory-bank/Diagnostic Report and Remediation Plan for LSTMCNN-LSTM Model Underperformance.md`, and `memory-bank/asset_id_embedding_strategy.md`.

## 4. Detailed Sub-Task Implementation Plans

---

### Sub-Task 1.3.1: Design Backtesting Strategy Class for NN Signals (`SupervisedNNStrategy`)

*   **Status:** Not Started
*   **Objective:** To design and document the `SupervisedNNStrategy` class, which will serve as the interface between trained supervised neural network models and the existing `core/backtesting` engine. This class will define how NN model predictions are translated into trading signals (BUY, SELL, HOLD) and actions within the backtesting framework.
*   **Scope:**
    *   **In Scope:** Defining class structure, methods, properties; specifying model/scaler loading; detailing signal generation logic (addressing continuous high probabilities); defining data feed interaction; outlining configuration parameters; documenting input/output formats.
    *   **Out of Scope:** Python implementation of the class; modification of the backtesting engine; NN model training.
*   **Prerequisites:**
    *   Task 1.2 Completion.
    *   Clear understanding of NN model outputs.
    *   Understanding of `core/backtesting` engine API.
    *   Access to relevant Memory Bank documents.
*   **Detailed Task Breakdown (for Design):**
    1.  **Review Existing `core/backtesting` Engine Interface:** Analyze strategy integration points, data feed, action execution.
        *   *Deliverable:* Summary of relevant backtester API.
    2.  **Define `SupervisedNNStrategy` Class Interface:** Draft Python class signature, methods (e.g., `__init__`, `on_bar_data`, `load_dependencies`, `generate_trade_action`), properties.
        *   *Deliverable:* Preliminary Python interface definition.
    3.  **Specify Model and Scaler Loading Mechanism:** Document loading of PyTorch model and feature scaler, including error handling.
        *   *Deliverable:* Specification for model/scaler loading.
    4.  **Design Signal Generation Logic:** Develop flowchart/pseudocode for transforming model probabilities to signals, handling continuous high probabilities, current position status, and max holding period.
        *   *Deliverable:* Detailed signal generation logic documentation.
    5.  **Define Interaction with Backtester's Data Feed:** Specify consumption of hourly bar data and features.
        *   *Deliverable:* Data feed interaction specification.
    6.  **List Configuration Parameters:** Identify `model_file_path`, `scaler_file_path`, `signal_threshold`, `max_holding_period_hours`, etc.
        *   *Deliverable:* List of configuration parameters.
    7.  **Document Input/Output Formats:** Define data structures for NN inputs within strategy and output signals to backtester.
        *   *Deliverable:* Input/output data format specification.
    8.  **Draft Initial Risk Management Hooks:** Outline integration of `max_holding_period`.
        *   *Deliverable:* Description of basic risk management integration.
    9.  **Consolidate Design:** Ensure all points are captured in a design document.
        *   *Deliverable:* Approved design document (e.g., `memory-bank/1.3.1_SupervisedNNStrategy_design_implementation_plan.md`).
*   **Technical Approach & Design Considerations (Summary - refer to the detailed plan for 1.3.1 for full details):**
    *   **Class:** `SupervisedNNStrategy` in `core/strategies/supervised_nn_strategy.py`.
    *   **Attributes:** `model`, `scaler`, `config`, `asset_id_map` (optional), `logger`.
    *   **Methods:** `__init__`, `load_dependencies`, `prepare_input_sequence`, `get_model_prediction`, `generate_trade_action`, `on_bar_data`.
    *   **Signal Logic:** Handle "FLAT" and "LONG"/"SHORT" states, incorporate `signal_threshold` and `max_holding_period_hours`.
*   **Key Deliverables:**
    *   A comprehensive design document for the `SupervisedNNStrategy` class. This was previously created as `memory-bank/1.3.1_SupervisedNNStrategy_design_implementation_plan.md`. This content should be considered part of this overall plan.
*   **Dependencies:** `core.backtesting` API, understanding of `core.data_preparation_nn` outputs.
*   **Testing Strategy Notes (for Design):** Review of the design document by peers/lead.
*   **Definition of Done:**
    1.  Design document for `SupervisedNNStrategy` is created, reviewed, and approved.
    2.  Design clearly outlines class structure, methods, data flow, and interactions.
    3.  Design addresses handling of continuous high-probability model outputs.
    4.  Design is sufficient for a developer to proceed with implementation.

---

### Sub-Task 1.3.2: Implement Trade Entry/Exit Rules Aligned with NN Signals

*   **Status:** Not Started
*   **Objective:** To implement the core logic within the `SupervisedNNStrategy` class for generating trade entry (LONG on BUY_SIGNAL) and exit signals (based on model output drop, time limit, or other defined rules).
*   **Scope:**
    *   **In Scope:** Coding the `generate_trade_action` method as per the design from 1.3.1. Implementing logic for BUY signals. Implementing exit logic based on:
        *   Model probability dropping below an `exit_threshold` (if defined).
        *   Reaching `max_holding_period_hours`.
        *   (Future) Potentially other explicit SELL signals from the model if it's multi-class.
    *   **Out of Scope:** Complex risk management rules (e.g., dynamic position sizing, stop-loss/take-profit based on price levels beyond the model's direct signal). Implementation of SHORT selling logic (initially focus on LONG and FLAT/EXIT).
*   **Prerequisites:**
    *   Completion and approval of Task 1.3.1 (Design of `SupervisedNNStrategy`).
    *   A skeleton or basic structure of the `SupervisedNNStrategy` class in `core/strategies/supervised_nn_strategy.py`.
*   **Detailed Sub-Task Breakdown:**
    1.  **Implement `generate_trade_action` Method Stub:** Create the method signature in the Python class.
    2.  **Implement Entry Logic (FLAT to LONG):**
        *   If current position is FLAT and model probability >= `signal_threshold`, generate "BUY".
    3.  **Implement Exit Logic (LONG to FLAT/SELL):**
        *   If current position is LONG and `time_in_position_hours` >= `max_holding_period_hours`, generate "SELL".
        *   If current position is LONG and model probability < `exit_threshold` (if `exit_threshold` is used and defined), generate "SELL".
        *   Consider if a "HOLD" signal from the model while in a LONG position means continue holding or if specific conditions trigger an exit. The design from 1.3.1 suggests HOLD if no exit condition is met.
    4.  **Implement HOLD Logic:** If no entry or exit conditions are met, generate "HOLD" or "DO_NOTHING".
    5.  **Unit Test Entry/Exit Logic:** Create unit tests covering various scenarios for `generate_trade_action`.
*   **Technical Approach & Design Considerations:**
    *   The logic will reside primarily in the `generate_trade_action` method of `SupervisedNNStrategy`.
    *   State management (current position, time in position) will be passed by the backtester to this method.
    *   Ensure clear distinction between entry signals and exit signals.
    *   Parameterize thresholds (`signal_threshold`, `exit_threshold`, `max_holding_period_hours`) via the strategy's configuration.
*   **Key Deliverables:**
    *   Implemented `generate_trade_action` method within `SupervisedNNStrategy`.
    *   Unit tests for the trade entry/exit logic.
*   **Dependencies:** Design from 1.3.1.
*   **Testing Strategy Notes:** Focus on unit testing `generate_trade_action` with mock inputs for probability, current position, and time in position.
*   **Definition of Done:**
    1.  `generate_trade_action` method is fully implemented according to the design.
    2.  Logic correctly handles entry based on `signal_threshold`.
    3.  Logic correctly handles exits based on `max_holding_period_hours` and potentially `exit_threshold`.
    4.  Unit tests for `generate_trade_action` achieve high coverage and pass.
    5.  Code is reviewed and merged.

---

### Sub-Task 1.3.3: Connect Model Inference to Backtester Data Feed

*   **Status:** Not Started
*   **Objective:** To implement the parts of `SupervisedNNStrategy` responsible for loading the NN model and scaler, preparing input data from the backtester's feed, and performing model inference at each time step.
*   **Scope:**
    *   **In Scope:** Implementation of `load_dependencies`, `prepare_input_sequence`, and `get_model_prediction` methods. Integration of these methods within the `on_bar_data` callback.
    *   **Out of Scope:** The backtester's data feed mechanism itself.
*   **Prerequisites:**
    *   Completion and approval of Task 1.3.1 (Design of `SupervisedNNStrategy`).
    *   Skeleton of `SupervisedNNStrategy` class.
    *   Availability of dummy/sample model and scaler files for testing.
*   **Detailed Sub-Task Breakdown:**
    1.  **Implement `load_dependencies` Method:** Code the logic to load PyTorch model and scikit-learn scaler from file paths provided in config. Include error handling.
    2.  **Implement `prepare_input_sequence` Method:** Code the logic to take the historical data window (DataFrame) from the backtester, select features, apply the loaded scaler, and reshape for model input. Handle asset ID if applicable.
    3.  **Implement `get_model_prediction` Method:** Code the logic to perform inference with the loaded model and prepared input sequence, returning the prediction probability.
    4.  **Integrate in `on_bar_data`:** Call these helper methods in the correct sequence within `on_bar_data`.
    5.  **Unit Test Each Method:** Create unit tests for `load_dependencies`, `prepare_input_sequence` (with mock data and scaler), and `get_model_prediction` (with a mock model).
*   **Technical Approach & Design Considerations:**
    *   Model and scaler loading should happen once during strategy initialization.
    *   Input preparation must be efficient as it's done at every bar.
    *   Ensure model is in `eval()` mode for inference.
    *   Handle potential CPU/GPU placement for the model.
*   **Key Deliverables:**
    *   Implemented `load_dependencies`, `prepare_input_sequence`, `get_model_prediction` methods.
    *   Functional `on_bar_data` method that orchestrates data prep and inference.
    *   Unit tests for these methods.
*   **Dependencies:** Design from 1.3.1, PyTorch, scikit-learn, pandas, numpy.
*   **Testing Strategy Notes:** Unit test methods individually. Mock external file dependencies (model, scaler) for robust tests.
*   **Definition of Done:**
    1.  `load_dependencies`, `prepare_input_sequence`, `get_model_prediction` methods are implemented and unit tested.
    2.  `on_bar_data` correctly calls these methods and passes data to `generate_trade_action`.
    3.  Strategy can successfully load a dummy model/scaler and process mock input data to produce a prediction.
    4.  Code is reviewed and merged.

---

### Sub-Task 1.3.4: Test Strategy on Sample Data with Dummy Model

*   **Status:** Not Started
*   **Objective:** To perform an initial integration test of the `SupervisedNNStrategy` with the `core/backtesting` engine using controlled sample data and a dummy NN model that produces predictable outputs.
*   **Scope:**
    *   **In Scope:** Creating a small, representative sample dataset (e.g., a few days of hourly data for one symbol). Creating a dummy PyTorch model that outputs fixed or easily verifiable probabilities based on input. Running the backtester with the `SupervisedNNStrategy` configured with this dummy setup. Verifying that trades are generated (or not) according to the dummy model's output and the strategy's logic.
    *   **Out of Scope:** Testing with fully trained, complex NN models. Exhaustive backtesting across many symbols or long periods.
*   **Prerequisites:**
    *   Completion of Tasks 1.3.2 and 1.3.3.
    *   Functional `core/backtesting` engine.
*   **Detailed Sub-Task Breakdown:**
    1.  **Create Sample Dataset:** Generate a small Parquet file with hourly OHLCV and necessary features for one symbol (e.g., AAPL for 1 week).
    2.  **Create Dummy Scaler:** Create and save a dummy scikit-learn scaler (e.g., a `StandardScaler` fit on some random data or an identity scaler).
    3.  **Create Dummy PyTorch Model:**
        *   Define a simple PyTorch `nn.Module` that takes the expected input sequence shape.
        *   Implement a `forward` method that returns a predefined probability (e.g., always 0.8, or 0.8 if input sum > X, else 0.2).
        *   Save this dummy model to a `.pt` file.
    4.  **Configure Backtester:** Set up a backtest run using the sample data, `SupervisedNNStrategy`, dummy model path, and dummy scaler path. Configure strategy parameters (thresholds, holding period).
    5.  **Run Backtest and Verify:** Execute the backtest. Examine logs and backtest results to confirm:
        *   Strategy initializes and loads dependencies correctly.
        *   Model inference occurs at each step.
        *   Trade signals ("BUY", "SELL", "HOLD") are generated as expected based on the dummy model's output and strategy logic.
        *   Trades are recorded by the backtester.
*   **Technical Approach & Design Considerations:**
    *   The dummy model should be simple enough that its output is easily predictable for given inputs.
    *   The sample data should include scenarios that test different branches of the `generate_trade_action` logic.
*   **Key Deliverables:**
    *   Sample data file(s).
    *   Dummy scaler file.
    *   Dummy PyTorch model file (`.pt`).
    *   A script or configuration for running the dummy backtest.
    *   A report/log confirming the expected behavior.
*   **Dependencies:** `core.backtesting` engine, implemented `SupervisedNNStrategy`.
*   **Testing Strategy Notes:** This task *is* a test. Focus on clear, verifiable outcomes.
*   **Definition of Done:**
    1.  Backtest with sample data and dummy model runs without errors.
    2.  Trade signals and executed trades match expectations based on the dummy model's predictable output and the strategy's defined logic.
    3.  Evidence (logs, results summary) of successful test execution is provided.

---

### Sub-Task 1.3.5: Integrate Backtesting Results Logging (PnL, Trade Count, etc.)

*   **Status:** In Progress
*   **Objective:** To ensure that the `core/backtesting` engine, when running the `SupervisedNNStrategy`, logs comprehensive results, including Profit and Loss (PnL), trade counts, and other relevant performance metrics, potentially integrating with MLflow as noted in `progress.md`.
*   **Scope:**
    *   **In Scope:** Verifying and, if necessary, enhancing the backtesting engine's existing logging capabilities to capture key metrics. Ensuring these metrics are accessible after a backtest run. If MLflow integration is intended, designing how parameters and metrics from backtests are logged to MLflow.
    *   **Out of Scope:** Building a new dashboard or visualization tool for results (unless MLflow UI is used). Implementing the MLflow tracking server setup itself.
*   **Prerequisites:**
    *   Functional `core/backtesting` engine.
    *   Completion of Task 1.3.4 (initial strategy test).
*   **Detailed Sub-Task Breakdown:**
    1.  **Review Existing Backtester Logging:** Analyze what metrics the `core/backtesting` engine currently logs (e.g., PnL per trade, total PnL, number of trades, win rate, max drawdown).
    2.  **Identify Missing Metrics:** Determine if any critical metrics for evaluating NN strategies are missing (e.g., Sharpe ratio, Sortino ratio, average holding period, signal-specific F1/precision/recall if applicable at strategy level).
    3.  **Enhance Backtester Logging (if needed):** Modify the backtesting engine to calculate and log any missing essential metrics.
    4.  **Design MLflow Integration (Conceptual):**
        *   Define which parameters (strategy config, model name) should be logged as MLflow parameters.
        *   Define which backtest results (total PnL, Sharpe, F1 of signals if possible, etc.) should be logged as MLflow metrics.
        *   Consider logging any artifacts (e.g., a summary CSV of trades, plots).
    6.  **Initial Debugging and Engine Stability (Completed as of 2025-05-23):**
        *   **Objective:** Resolve runtime errors preventing the `scripts/run_dummy_backtest.py` from executing successfully with the `BacktestingEngine`.
        *   **Issues Addressed:**
            *   `TypeError: Cannot compare tz-naive and tz-aware datetime-like objects` in `core/backtesting/data.py`: Resolved by localizing DataFrame index to UTC (`df.index = df.index.tz_localize('UTC')`).
            *   `AttributeError: 'generator' object has no attribute '__self__'` in `core/backtesting/data.py`: Resolved by storing the full DataFrame in `self.symbol_data` before creating an iterator, and accessing the DataFrame directly for operations like `self.symbol_data[s]['close']`.
            *   `NameError: name 'time' is not defined` in `core/backtesting/engine.py`: Resolved by adding `import time`.
            *   `FutureWarning: DataFrame.fillna with 'method' is deprecated` in `core/backtesting/data.py`: Resolved by changing `fillna(method='ffill')` to `ffill()` and `fillna(method='bfill')` to `bfill()`.
        *   **Outcome:** `scripts/run_dummy_backtest.py` now runs to completion (exit code 0) without errors or warnings, with MLflow logging attempted. This establishes a stable baseline for further development of metrics and strategy integration.
    5.  **Implement MLflow Logging Hooks (if feasible within scope):** Add `mlflow.log_param`, `mlflow.log_metric`, `mlflow.log_artifact` calls at appropriate places in the backtesting script/engine.
*   **Technical Approach & Design Considerations:**
    *   Logging should be configurable (e.g., log level, MLflow experiment name).
    *   Metrics should be clearly defined and consistently calculated.
    *   MLflow integration should be optional or configurable to not break runs if MLflow is not set up.
*   **Key Deliverables:**
    *   Updated backtesting engine with comprehensive results logging.
    *   (If implemented) Integration points for MLflow logging.
    *   Documentation on available logged metrics and how to access them.
*   **Dependencies:** `core.backtesting` engine, MLflow library (if integrated).
*   **Testing Strategy Notes:** Run backtests (e.g., the dummy test from 1.3.4) and verify that all expected metrics are logged correctly to console output, files, and/or MLflow.
*   **Definition of Done:**
    1.  Backtesting engine logs key performance metrics (PnL, trade count, win rate, Sharpe ratio, max drawdown).
    2.  (If MLflow integrated) Backtest parameters and metrics are successfully logged to MLflow.
    3.  Documentation for logged metrics is available.
    4.  Code is reviewed and merged.

---

### Sub-Task 1.3.6: Validate Backtester with Edge Cases (No Signals, Constant Signals)

*   **Status:** Not Started
*   **Objective:** To rigorously test the `SupervisedNNStrategy` and backtesting engine integration with edge case scenarios, such as a model that produces no "BUY" signals or a model that produces constant "BUY" signals.
*   **Scope:**
    *   **In Scope:** Creating dummy models or configuring the `SupervisedNNStrategy` to simulate these edge cases. Running backtests for these scenarios. Verifying the system behaves correctly (e.g., no trades for "no signals", trades according to `max_holding_period` for "constant signals").
    *   **Out of Scope:** Testing all possible edge cases; focus on these two critical ones.
*   **Prerequisites:**
    *   Completion of Task 1.3.4 and 1.3.5.
*   **Detailed Sub-Task Breakdown:**
    1.  **Scenario 1: No "BUY" Signals:**
        *   Create/configure a dummy model that always outputs a probability below the `signal_threshold`.
        *   Run a backtest with this setup.
        *   **Verify:** No BUY trades are executed. Portfolio value remains unchanged (or only changes due to cash interest if modeled). Logs reflect no BUY signals.
    2.  **Scenario 2: Constant "BUY" Signals:**
        *   Create/configure a dummy model that always outputs a probability above the `signal_threshold`.
        *   Run a backtest with this setup.
        *   **Verify:** A BUY trade is initiated at the first opportunity. The position is held. An exit occurs due to `max_holding_period_hours`. A new BUY trade may be initiated immediately after if the signal persists. The system doesn't crash or enter an infinite loop. Trade logs reflect this pattern.
    3.  **Document Test Cases and Results:** Record the setup and outcomes for these edge case tests.
*   **Technical Approach & Design Considerations:**
    *   These tests specifically target the robustness of the `generate_trade_action` logic and its interaction with the backtester's position management.
*   **Key Deliverables:**
    *   Test scripts/configurations for edge case scenarios.
    *   A report summarizing the results and confirming correct behavior.
*   **Dependencies:** Implemented `SupervisedNNStrategy`, `core.backtesting` engine.
*   **Testing Strategy Notes:** This task *is* a test. Ensure clear pass/fail criteria.
*   **Definition of Done:**
    1.  Backtests for "no signals" and "constant signals" scenarios run successfully.
    2.  The system behaves as expected in both scenarios (no trades, or trades limited by holding period).
    3.  Test results are documented and confirm system robustness.

---

### Sub-Task 1.3.7: Integration with Supervised NN Strategy (Phase 1 Baseline)

*   **Status:** Not Started
*   **Objective:** To fully integrate the developed `SupervisedNNStrategy` as the primary strategy for backtesting the baseline supervised NN models developed in Phase 1 (Task 1.4).
*   **Scope:**
    *   **In Scope:** Ensuring that the backtesting framework can seamlessly use the `SupervisedNNStrategy` with actual (even if preliminary) trained NN models from Task 1.4. This involves configuring the backtester to use this strategy by default for NN model evaluations. Refining any minor issues found during initial integration with real models.
    *   **Out of Scope:** The training of the NN models themselves. Extensive performance evaluation of the models (that's part of 1.4.5).
*   **Prerequisites:**
    *   Completion of Tasks 1.3.1 through 1.3.6.
    *   At least one preliminary trained supervised NN model and its corresponding scaler from Task 1.4 (or well-defined dummy versions that mimic real model I/O).
*   **Detailed Sub-Task Breakdown:**
    1.  **Obtain Preliminary Trained Model/Scaler:** Get artifacts from Task 1.4.
    2.  **Configure Backtest with Real Model:** Set up a backtest run using the `SupervisedNNStrategy`, pointing to the actual model and scaler files.
    3.  **Execute Initial Backtest Runs:** Run backtests on a few symbols or a defined period.
    4.  **Debug and Refine:** Identify and fix any issues arising from the integration (e.g., data mismatches, unexpected strategy behavior with real model outputs).
    5.  **Confirm Workflow:** Ensure the end-to-end workflow (load data -> strategy processes -> model predicts -> strategy signals -> backtester trades -> results logged) is smooth.
*   **Technical Approach & Design Considerations:**
    *   This step is about ensuring the "plumbing" works correctly with real-world (or near real-world) components.
    *   Pay attention to logging to trace data flow and decision points.
*   **Key Deliverables:**
    *   A successfully executed backtest run using `SupervisedNNStrategy` with a preliminary trained NN model.
    *   Documentation of any integration issues found and their resolutions.
*   **Dependencies:** `SupervisedNNStrategy`, `core.backtesting` engine, preliminary outputs from Task 1.4.
*   **Testing Strategy Notes:** Focus on successful execution and basic sanity checks of outputs.
*   **Definition of Done:**
    1.  The `SupervisedNNStrategy` can be successfully configured and run with preliminary trained NN models within the backtesting engine.
    2.  The backtesting process completes without errors related to the strategy or model integration.
    3.  Basic trade generation and results logging are confirmed with a real (or realistic dummy) model.

---

### Sub-Task 1.3.8: Implementation of Advanced Validation Techniques

*   **Status:** Not Started 
*   **Objective:** To finalize and fully integrate advanced validation techniques into the backtesting and strategy evaluation workflow. This includes robust calculation of multiple performance metrics and methods for determining optimal decision thresholds.
*   **Scope:**
    *   **In Scope:**
        *   Reviewing and completing any "initial support" for multiple metrics (e.g., F1, Precision, Recall for signals, Sharpe Ratio, Sortino Ratio, Calmar Ratio, Max Drawdown, average PnL per trade, win/loss ratio).
        *   Implementing or finalizing a method for determining an optimal classification/signal threshold (e.g., based on maximizing F1-score on a validation set, or maximizing a profit-based metric from a validation backtest).
        *   Ensuring these advanced metrics and threshold optimization results are logged (including to MLflow if integrated).
        *   Integrating these techniques into the standard evaluation protocol for `SupervisedNNStrategy`.
    *   **Out of Scope:** Developing entirely new statistical validation methods not previously considered. Complex walk-forward optimization schemes (basic walk-forward testing is assumed part of the backtester, but optimizing parameters *across* walk-forward windows is more advanced).
*   **Prerequisites:**
    *   Completion of Task 1.3.5 (basic results logging).
    *   A functional `SupervisedNNStrategy` and backtesting setup.
    *   Understanding of the "initial support" already implemented.
*   **Detailed Sub-Task Breakdown:**
    1.  **Review Existing "Initial Support":** Understand what metrics and threshold optimization methods are already partially in place.
    2.  **Finalize Metric Calculation:** Ensure all desired advanced metrics are correctly calculated and robustly implemented within the backtester or as a post-processing step on backtest results.
        *   Metrics: Sharpe, Sortino, Calmar, Max Drawdown, Avg PnL/trade, Win Rate, Profit Factor.
        *   If applicable at strategy level: Precision, Recall, F1 for BUY signals generated.
    3.  **Finalize Threshold Optimization:**
        *   Implement a utility or method that can take validation set predictions (probabilities) and true labels (or validation backtest outcomes) to find an optimal `signal_threshold`.
        *   This could involve iterating through thresholds and selecting the one that maximizes a chosen metric (e.g., validation F1, or validation backtest PnL).
    4.  **Integrate into Evaluation Workflow:**
        *   Ensure that after a backtest (especially a validation run), these advanced metrics are computed and displayed/logged.
        *   If threshold optimization is performed, the optimal threshold and its corresponding validation performance should be logged.
    5.  **Update Logging:** Ensure all new metrics and optimized thresholds are logged to console, files, and MLflow (if used).
    6.  **Document Advanced Validation:** Update documentation to explain the available advanced metrics and how threshold optimization can be used.
*   **Technical Approach & Design Considerations:**
    *   Metric calculations can leverage libraries like `empyrical` or custom implementations.
    *   Threshold optimization might involve a simple loop or a more sophisticated method if optimizing a complex objective.
    *   Consider making the threshold optimization step optional or configurable.
*   **Key Deliverables:**
    *   Fully implemented and tested calculation of advanced performance metrics.
    *   A functional mechanism for optimizing the signal threshold based on validation performance.
    *   Updated logging to include these advanced metrics and optimized thresholds.
    *   Documentation for these advanced validation features.
*   **Dependencies:** `core.backtesting` engine, `SupervisedNNStrategy`, potentially libraries like `empyrical`.
*   **Testing Strategy Notes:**
    *   Test metric calculations with known trade sequences to verify correctness.
    *   Test threshold optimization with sample prediction data to ensure it selects a reasonable threshold.
    *   Verify that all metrics and optimized values are correctly logged.
*   **Definition of Done:**
    1.  A comprehensive set of advanced performance metrics is calculated and logged by the backtesting system.
    2.  A mechanism for optimizing the `signal_threshold` for `SupervisedNNStrategy` is implemented and functional.
    3.  Results of advanced validation (metrics, optimal threshold) are clearly reported and logged (including MLflow if applicable).
    4.  Documentation for these features is updated.
    5.  Code is reviewed and merged.

---

This comprehensive plan should guide the development team through the "1.3 Build Backtesting Engine Integration" phase.