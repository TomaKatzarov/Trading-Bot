Neural Network Architectures for Mid-Horizon Financial Forecasting (3–5% Returns)
Introduction
Predicting mid-horizon price movements (e.g. targeting 3–5% returns over a trade’s holding period) is a challenging task in financial time-series forecasting. Such price swings often unfold over several hours to days, making them distinct from both ultra short-term “next-tick” fluctuations and long-term trends. The use case at hand involves an autonomous trading system (integrated with the Alpaca API) that ingests multivariate time-series features – 9 technical indicators and 1 sentiment score – and produces trading signals when an actionable return threshold (3–5%) is in sight. This setting demands supervised neural network models that can learn complex patterns from historical data and operate in real-time (but not high-frequency) conditions. Over the past decade, deep learning has markedly improved financial forecasting accuracy, often outperforming traditional models like ARIMA or SVM
techscience.com
techscience.com
. However, financial data presents unique challenges: non-stationarity (changing statistical properties over time), low signal-to-noise ratio, and the risk of overfitting spurious patterns. Different neural architectures address these issues with varying success. This review examines four major architecture families in this context – Multi-Layer Perceptrons (MLPs), Recurrent Neural Networks (especially LSTMs/GRUs and bidirectional variants), Transformer-based models, and 1D Convolutional Neural Networks (CNNs) – evaluating each in terms of strengths, weaknesses, input representation techniques, typical challenges, and empirical performance on mid-horizon financial forecasting tasks. We also discuss the technical indicators most pertinent to each architecture and why certain features (trend, momentum, volatility, volume, sentiment, etc.) can enhance predictive accuracy. Below is a comparative overview of these architectures:
Architecture	Strengths	Weaknesses
Multi-Layer Perceptron (MLP)	– Simple and fast to train
– Handles non-linearity in features well
– Requires no sequence-specific tooling	– No inherent memory of temporal order
– Needs fixed-size input window
– Limited ability to capture long patterns
Recurrent NN (LSTM/GRU)	– Remembers temporal dependencies via hidden state
– Handles long sequences better (LSTM/GRU mitigate vanishing gradients)
techscience.com

– Proven effective in many financial tasks
techscience.com
techscience.com
– Sequential processing (slower training)
– Can still struggle with very long-term dependencies without modifications
techscience.com

– Risk of overfitting noisy data without regularization
Transformer (Attention)	– Self-attention captures long-range relationships
techscience.com

– Parallelizable sequence processing (faster inference/training for long windows)
– Flexible integration of multiple features (multivariate attention)	– High complexity (many parameters)
– Needs large data or strong regularization to avoid overfitting
– Memory intensive for long sequences (naïve attention is O(n²))
– Less proven on small/medium datasets compared to LSTM
1D Convolutional NN (CNN)	– Extracts local temporal patterns efficiently
– Fast to train (convolutions are parallelized)
– Natural noise filtering by smoothing over time
– Can handle multivariate inputs as channels	– Fixed receptive field per layer (no innate long-term memory)
– Requires deep or dilated convolutions for long-range patterns
techscience.com

– Cannot inherently account for order beyond local window (needs combination with RNN or attention for long sequence context)

Each architecture is detailed in the following sections. We highlight how the input data can be structured for it (e.g. sliding windows, feature scaling), the typical pitfalls when applying it to financial data (like non-stationarity, overfitting, or gradient issues), and representative results from literature on similar mid-horizon forecasting problems.
Multi-Layer Perceptrons (MLPs) – Baseline Dense Networks
Overview: The MLP is a classical feed-forward neural network consisting of fully-connected layers. It treats input data as a fixed-size feature vector, making no special assumptions about sequence or time. In financial forecasting, an MLP is often used as a baseline model to benchmark performance of more complex architectures. For example, Nikou et al. (2019) included an MLP when predicting daily UK stock prices, and found it underperformed an LSTM and even some non-neural models
techscience.com
. Despite its simplicity, an MLP can model non-linear relationships between technical indicators and future returns to a degree, and is fast to train and evaluate. Strengths: MLPs are straightforward to implement and computationally inexpensive. They excel at learning non-linear combinations of input features. If the input feature vector already captures relevant temporal patterns (through lagged values or technical indicators), an MLP can approximate the mapping from those features to the target without needing sequence memory. MLPs have well-organized structures and can “automatically extract subtle patterns” from a sufficiently large dataset by layering multiple neurons
annals-csis.org
. In practice, an MLP with even one or two hidden layers can serve as a robust baseline for financial predictions, often outperforming linear models like ARIMA or simple regressions on the same features
techscience.com
. Another advantage is reduced risk of training instabilities – unlike deep recurrent networks, MLPs don’t suffer from time-step-wise gradient vanishing/exploding, and their training converges quickly. They are also easier to regularize (via weight decay or dropout) due to their simple structure. Weaknesses: The chief limitation is the lack of temporal memory. An MLP has no notion of sequence or order – it cannot natively model how yesterday’s output relates to today’s input, for example. It requires a predetermined window of past data to be fed as part of the input vector. If important patterns span longer than the chosen window, the MLP may miss them. Even within a window, an MLP treats each position as a separate feature, lacking the translational invariance of CNNs or the adaptive memory of RNNs. This means an MLP might need many input features (lags, indicators, etc.) to indirectly capture time dynamics, which can make the model unwieldy. Moreover, financial time-series are noisy; an MLP might overfit to random fluctuations if it has too many parameters relative to the data size. Regularization and cross-validation are critical to ensure it generalizes. In comparative studies, MLPs often yield lower accuracy than sequence-based models; for instance, one analysis showed simple RNNs or CNNs achieving lower forecast error than an MLP in one-day stock predictions
annals-csis.org
annals-csis.org
. Thus, while an MLP sets a baseline, it typically underperforms more specialized architectures on capturing temporal structures. Input Representation: To use an MLP for time-series forecasting, the sequential data must be converted into a fixed-size feature vector. A common approach is sliding window feature extraction: e.g., take the past N observations (prices or indicators over the last N time steps) and flatten them into a 1×N vector as input, with the output being the prediction of a future value or signal. In the user’s scenario (9 technical indicators + 1 sentiment), one could concatenate the indicators and sentiment of the last N periods. For example, if a 5-day window is used, the input might be 5×10 = 50 features long. This fixed-window approach allows an MLP to ingest temporal information, albeit in a rudimentary way. Because different features have different scales (prices, oscillators, volumes, sentiment scores), feature scaling is essential – e.g. z-score normalization or min-max scaling – to stabilize learning. Many practitioners also difference or detrend the data (using returns or percent changes instead of raw prices) to make it more stationary before feeding to an MLP
techscience.com
. MLPs themselves don’t enforce stationarity, so it’s up to preprocessing to remove trends that could otherwise confuse the network. Dimensionality reduction or feature selection can help if the window is large; for instance, using only a few salient technical indicators or principal components rather than dozens of highly correlated inputs. Challenges in Financial Data: When applying MLPs to financial time series, one faces the general challenges of any machine learning on financial data – non-stationarity, noise, and overfitting – compounded by the MLP’s lack of built-in sequence handling. Non-stationarity means the relationships between indicators and returns can change over time (regime shifts). An MLP trained on one period might falter if market dynamics change; retraining or using rolling windows for training may be necessary to keep the model current
techscience.com
. The abundance of noise (random price fluctuations) can lead the MLP astray: with enough capacity it may memorize random quirks of the training sample. This is mitigated by keeping the network shallow or using strong regularization (dropout, ridge penalties) and by including only meaningful features (indicators that reflect robust market insights). Overfitting is a constant risk – the MLP might appear to fit training data extremely well but fail on new data. Rigorously evaluating on out-of-sample data and employing early stopping is important
techscience.com
. Another challenge is choosing the correct input window length: too short may omit important build-up to a 3–5% move; too long may add irrelevant noise and make the input too high-dimensional for the MLP to generalize. Since there is no internal state, the MLP won’t “remember” anything beyond what is explicitly in the input vector, so the window must cover the essential lead-up to the target event. Practitioners often experiment with different window sizes (e.g. 5, 10, 20 days) and evaluate which yields the best validation performance
annals-csis.org
. Empirical Performance: In literature, MLPs have shown mediocre performance on complex financial prediction tasks compared to deep sequence models
techscience.com
annals-csis.org
. For instance, in one study of 20 S&P500 stocks over 7 years, a simple RNN and a CNN each outperformed an MLP in most cases for one-day-ahead price forecasting
annals-csis.org
annals-csis.org
. The MLP tended to have higher prediction errors, indicating it struggled to capture the time dependencies that RNN/CNN could. Another review noted that before deep learning rose to prominence, ensemble trees (Random Forests) or SVMs often beat neural nets like MLP on stock trend classification
techscience.com
 – suggesting that a basic MLP was not the method of choice when more sophisticated sequence models were unavailable. That said, MLPs still appear in recent research as benchmarks and sometimes as components of hybrid models. They can perform reasonably well in stable market regimes or when extensive feature engineering (with technical indicators) has distilled the predictive signals. For example, an evolutionary deep learning study by Alhaj-Yaseen et al. (2020) found an optimized MLP could detect stock trend changes when using a rich set of technical indicator inputs
researchgate.net
dr.library.brocku.ca
. Overall, an MLP provides a useful baseline (Level-0): if it cannot achieve at least some skill with the given inputs, more complex models likely won’t magically succeed either. In our mid-horizon use case, one would expect an MLP to yield some predictive power (especially due to the informative technical and sentiment features), but it will likely be outperformed by sequence-aware architectures that we discuss next.
Recurrent Neural Networks – LSTMs, GRUs, and Variants
Overview: Recurrent Neural Networks (RNNs) are specifically designed to handle sequence data by maintaining an internal state (memory) that evolves with each time step. They recurrently apply the same network cell at each step, processing inputs sequentially. For financial time series, RNNs (especially the gated variants like LSTM and GRU) have become a go-to architecture, as they naturally incorporate the temporal order of data. Long Short-Term Memory (LSTM) networks introduce gating mechanisms to preserve long-term information and mitigate the vanishing gradient problem that plagues “vanilla” RNNs
techscience.com
. Gated Recurrent Units (GRUs) are a streamlined variant of LSTM with fewer gates, often comparable in performance but faster to train. Bidirectional RNNs process sequence both forward and backward in time, which can be useful in offline analysis (e.g. denoising or pattern recognition) but are not applicable to live trading predictions since future data is not available in real time. In short, recurrent networks can learn the dynamic temporal patterns leading up to a 3–5% price move by storing relevant information in their state over dozens of time steps. Strengths: The primary strength of RNNs is their ability to capture temporal dependencies of varying lengths. An LSTM/GRU can, in theory, learn that a certain combination of indicator signals over the past T days increases the probability of a 5% upward move on day T+1. Unlike an MLP, the RNN doesn’t need the input window to explicitly contain all interactions; it can integrate information over time via its hidden state. LSTMs are known for handling long sequences far better than vanilla RNNs – their forget/input/output gates regulate information flow such that important signals can persist and irrelevant ones can be dropped
techscience.com
. This property is vital in finance: meaningful patterns (like a momentum buildup or a reversal signal) might span multiple days or weeks, and LSTM memory cells help remember early triggers until they materialize in price movement. In practice, LSTM has emerged as the dominant RNN variant in financial forecasting
techscience.com
. It has been reported to outperform not only MLPs but also traditional models like SVR and Random Forest in various studies
techscience.com
. For example, Fischer & Krauss (2018) found an LSTM network achieved higher Sharpe ratio and accuracy than random forests when predicting daily S&P500 stock movements, highlighting the advantage of LSTM in capturing long-term dependencies that tree models couldn’t. GRUs, while simpler, often match LSTM performance on mid-length sequences; some researchers choose GRU for faster training when data is limited. Another strength is that RNNs can handle variable-length input sequences gracefully. With techniques like padding or dynamic computation, an RNN model could be trained on, say, 60-day sequences but also make predictions on a 30-day sequence if needed (padding shorter ones or truncating longer ones). This is useful if different trade setups naturally have different lead times. Additionally, RNNs can incorporate exogenous inputs smoothly: at each time step the input vector can include technical indicators, sentiment scores, or other series, allowing the model to learn their joint temporal influence. Overall, RNNs offer a powerful framework for modeling time-series without heavy manual feature engineering – they learn the time lags and pattern shapes that matter. Weaknesses: Despite their sequential prowess, RNNs have several challenges. First, training can be slow and tricky due to the sequential nature of backpropagation (Backprop Through Time). Unlike CNNs or MLPs, you cannot fully parallelize RNN computation for a given sequence, which increases training time especially for long sequences. They also have a notoriously large number of hyperparameters (number of layers, hidden units, learning rate, sequence length, etc.) which require careful tuning. While LSTMs mitigate gradient decay, they are not immune to very long-term dependency issues – if one tries to look back hundreds of time steps, even LSTMs may struggle or need adjustments like attention mechanisms
techscience.com
. There is also the issue of gradient exploding, where a rare long-term dependency causes gradients to blow up; this is usually managed by gradient clipping and small learning rates
techscience.com
. Another weakness is that RNNs can overfit easily if they have more capacity than the data can support. They will memorize sequences (especially if many indicators are used) and fail to generalize. Regularization techniques like dropout (applied to RNN hidden states) are often used to combat this
techscience.com
. RNNs can sometimes be too flexible and latch onto spurious correlations, essentially amplifying biases present in the training data
techscience.com
 (for example, if most training examples of a 5% rally happened during rising interest rate regimes, the RNN might implicitly become biased to that regime). Ensuring the training data is diverse and representative is crucial. Another practical weakness: preparing input data for RNNs is a bit more involved – one has to shape the training set as sequences (with appropriate labels) and often pad sequences to a common length or use stateful iteration. This “data formatting” overhead is noted as a minor downside in some surveys【46†look 192 488 704】 (the table indicates preparing sequential input can be difficult). Finally, while not a weakness per se, the lack of interpretability in RNNs is often mentioned – it’s hard to extract why the network made a prediction (though techniques like attention or gradient attribution can help). In summary, RNNs are powerful but complex to train, requiring careful design to avoid traps like long-term dependency failure or simply learning “autocorrelation” instead of true pattern (one study observed that LSTM sometimes falls into predicting based on recent autocorrelation and misses more subtle relations
atlantis-press.com
). Optimal Input Representation: RNNs expect a sequence of input vectors. For our use case, this means at each time step t, the model could receive a vector of the 10 features (technical indicators + sentiment). A common setup is to use a rolling window of length T as the RNN input sequence (where T might be 20 days, 60 minutes, etc., depending on how far back we believe the model needs to see to predict a 3–5% move). Each sequence would be labeled with the target signal – for example, a binary label whether a +3% rise occurs within the next K periods, or a regression target of the next period’s return. It is crucial to scale the inputs (RNNs benefit from normalized inputs to prevent one feature from dominating; e.g., scale indicators to 0–1 or standardize to mean 0, std 1). Many studies feed differentiated series to RNNs – instead of raw price, use returns or percentage changes – to handle non-stationarity
techscience.com
. The internal gating of LSTM will handle baseline shifts better if the data oscillates around a relatively stable mean (which returns do). For multivariate input, the RNN will treat the indicator set at time t as one composite input vector. One can also include time-step embeddings or flags (like day-of-week or other seasonal info) as additional inputs at each step if relevant. Bidirectional RNNs (Bi-LSTM) deserve a note: during training, they process the sequence forward and backward, effectively using both past and future context. Some researchers use Bi-LSTMs to exploit full knowledge of a historical sequence for making a classification (for instance, identifying chart patterns or post-hoc predicting whether a past move was a true positive or a false signal)
ijsrd.com
. In fact, one experiment found that a Bi-LSTM achieved lower RMSE than a standard LSTM on a stock price series, due to its “superior capability to prevent overfitting with more context”
ijsrd.com
ijsrd.com
. However, in live trading we can’t use future data, so Bi-RNNs would be confined to offline analysis or as part of an ensemble for evaluation. For training efficiency, sequences are often batched (padded to equal length and run in parallel), and techniques like masking ensure the model ignores the padding. In financial data, event sequences can be of different lengths (some 5% moves build up in 3 days, others in 10 days); a strategy is to train the RNN on a maximum window (say 60 days) and pad shorter sequences, or train on fixed-length sequences but use a sliding prediction scheme at inference. Challenges in Financial Applications: RNNs face notable challenges when modeling financial time series. Non-stationarity and regime shifts can particularly hurt RNNs: since they learn a single set of weights for all time, if the market’s behavior changes, the patterns the RNN learned (like the signature of a rally) may no longer apply. This can cause the RNN’s memory to be “wrong-footed” – it might expect a outcome based on past pattern that doesn’t hold in the new regime. Continual training or periodic retraining on recent data is often needed
techscience.com
. Overfitting and generalization are constant worries; an LSTM with many neurons can memorize specific sequences (especially if using many technical indicators, which might allow it to effectively memorize historical outcomes by indicator configurations). Proper cross-validation, regularization, and early stopping are needed. In addition, because of the heavy use of past data, label leakage is a subtle pitfall – one must ensure that when creating sequences for training, the target (e.g. occurrence of 3% move) is strictly in the future of the input sequence. Any data preprocessing that inadvertently uses future data (such as indicator calculation that uses future bars, or an improperly aligned sentiment score) can give the RNN an unrealistic advantage. Class imbalance can also be an issue: if only, say, 5% of the time windows lead to a ≥3% return event, a naive RNN might just learn to always predict “no event” and achieve 95% accuracy. To combat this, one might oversample event sequences or use a weighted loss. Moreover, RNNs can be sensitive to input scaling and initialization – poor scaling can slow training or lead to bad local minima. It’s advised to use techniques like layer normalization or careful weight initialization to stabilize training for long sequences. As noted earlier, RNNs are computationally heavier: processing a long sequence (say 100 time steps) involves 100 sequential operations per layer. This is usually fine for daily or hourly data, but for high-frequency data even LSTMs become impractical without significant hardware (not our case here, as we’re not HFT). Lastly, interpretability: with technical indicators as inputs, one might want to know which signals the RNN is relying on. This isn’t straightforward – one could examine input-output sensitivity or use techniques like SHAP, but the temporal aspect complicates things. Some researchers integrate attention mechanisms with RNNs to get an idea of which time steps are being focused on (more on this in the Transformer section), which can double as a solution for capturing very long-term effects that vanilla LSTM can’t retain. Empirical Performance: RNN-based models, especially LSTMs, have shown strong performance on a variety of financial prediction tasks. They are often considered a state-of-the-art baseline in academic literature
techscience.com
. Empirically, LSTMs have outperformed older models in forecasting stock prices, index levels, and even macroeconomic time series. As an example, Chen et al. (2015) found LSTM networks gave more accurate regression of stock prices than ANN or SVR models
techscience.com
. Hoseinzade & Haratizadeh (2019) applied LSTM to limit-order book data and beat complex feature-based methods, demonstrating LSTM’s ability to extract signal from raw sequential data. For mid-horizon signals like our 3–5% return goal, there have been studies on trend prediction where LSTMs achieved higher F1-scores or returns than alternatives. Nelson et al. (2017) used an LSTM to predict stock trend (up/down) from technical indicators and got around 55-60% accuracy, exceeding random chance and simpler models. In a more directly relevant study, a 2024 ensemble RNN approach by Saud & Shakya tested GRU and LSTM networks with trend indicators (MACD, DMI, KST) for next-day stock prediction. They found that individually, an LSTM using the KST momentum indicator was the best single-model predictor, and an ensemble of 3 GRUs slightly outperformed an ensemble of LSTMs
oaji.net
oaji.net
. This underscores that LSTM/GRU models are effective, and even a GRU (with fewer parameters) can sometimes edge out LSTM, likely by being less prone to overfitting in that scenario
oaji.net
. On the other hand, there are also cases where LSTM did not significantly outperform a well-tuned traditional approach. For example, Zhang et al. (2023) noted that while LSTMs generally have lower error than ARIMA on stock index forecasting, they sometimes only marginally beat simpler models on very noisy series (the LSTM might predict just the autocorrelation, as mentioned in the A-share study)
atlantis-press.com
. Notably, the A-share stock study found LSTM had better error metrics than a Transformer, but the authors still saw potential in the Transformer capturing relationships LSTM missed
atlantis-press.com
. In summary, LSTMs/GRUs are usually top performers for supervised mid-horizon forecasting, especially when ample training data is available. They strike a good balance between capacity (to learn complex patterns) and bias (they inherently structure the problem as sequential, which is a correct bias for time-series). Our expectation for the use case is that an LSTM or GRU-based model will significantly outperform a naive MLP, and serve as a strong foundation. Indeed, hybrid models that include LSTM components (like CNN-LSTM or LSTM with attention) have been repeatedly shown to exceed the performance of any single model architecture
techscience.com
. This makes a plain LSTM a sensible Phase 1 choice, with the option to expand to hybrid or attention-augmented versions in future phases.
Transformer-Based Models – Attention for Financial Time Series
Overview: Transformer models have revolutionized sequence modeling in NLP and are increasingly being applied to time-series forecasting, including finance. The hallmark of Transformers is the self-attention mechanism, which allows the model to weigh the relevance of different time steps when making a prediction. In contrast to RNNs, Transformers do not process data strictly sequentially; instead, they look at the entire input sequence (often with positional encodings to retain order information) and determine which parts of the sequence to focus on. For financial forecasting, especially with medium-sized datasets (~300k samples), researchers are experimenting with custom, smaller-scale Transformers rather than the gigantic pre-trained models used in NLP. Examples include tailored architectures like Temporal Fusion Transformer (TFT), or simplified encoder-decoders for time series, and approaches such as Informer, Autoformer, and other efficient Transformer variants that handle long series with fewer resources. The motivation is that attention could detect patterns like “this week’s indicator configuration is similar to a pattern 2 months ago before a 4% jump” – something an LSTM might not explicitly recall if the gap is long. By using attention, the model can directly connect distant relevant signals with the present. Strengths: The biggest strength of Transformer models is their ability to capture long-range dependencies and complex relationships in the data. Because self-attention examines pairwise interactions between time points, a Transformer can learn that, for example, “a sharp rise in bullish sentiment 10 days ago coupled with steadily increasing volume until now” is a strong precursor of a price breakout. It does not inherently forget older inputs as RNNs might; every input’s representation can attend to every other input (within the chosen window) with varying weight. This is extremely useful in finance where sometimes distant historical events (e.g., a past support level, or a prior earnings surprise) can influence the current move. Attention mechanisms have been shown to improve modeling of long sequences without blowing up model size
techscience.com
 – by focusing on the truly relevant parts of the history, the model effectively compresses important information. For mid-horizon predictions, this means a Transformer could glean which past days’ market conditions most resemble the current situation and weigh them more in its forecast. Another strength is parallelism in training and inference. Unlike RNNs that must iterate step by step, Transformers compute attention for all time steps simultaneously (the only sequential part is adding positional embeddings and the fact that for autoregressive generation you’d mask future positions – but for our predictive use, we can input the whole known window at once). This makes training faster for long sequences on GPU, and enables using larger receptive fields (e.g. 100 days) that might be infeasible for an LSTM to remember. Moreover, Transformers naturally handle multivariate inputs: they can attend not just across time steps but also across different feature dimensions if set up appropriately. Many implementations treat the multivariate time series as a sequence of feature vectors (like RNNs do), and the attention layers implicitly learn cross-feature interactions at each time step as well as time-time interactions. Some advanced designs even use separate attention heads for different features or encoder-decoder structure where one encoder processes, say, historical technical indicators, and another processes a sentiment sequence, and then a decoder layer attends between the two to make a prediction. This flexibility in architecture is a plus. Transformers have also shown strength in representation learning – they can learn subtle patterns that might span multiple time scales (for instance, a short-term momentum combined with a long-term trend). Empirically, Transformers have begun to match or exceed LSTM performance in several time-series domains, especially as data volumes increase
atlantis-press.com
researchgate.net
. A 2023 study reported that a Transformer model outperformed an LSTM and even Prophet (a statistical model) in predicting stock prices of certain companies
dl.acm.org
. Additionally, attention outputs can provide a form of interpretability: one can inspect the attention weights to see which past days the model focused on for its prediction, offering insights to human analysts (though attention isn’t a perfect explainer, it’s still useful for explainable AI in finance). Finally, for our moderate data size (~300k samples), a “small Transformer” (few layers, moderate embedding size) is feasible to train from scratch in a reasonable time, making it a viable option in an autonomous trading system. Weaknesses: The benefits of Transformers come with trade-offs. The primary concern is their complexity and data hunger. Transformers typically have a lot of parameters (weights in queries, keys, values for multiple attention heads, plus feed-forward network layers after attention). If the dataset is not large enough, they can overfit more easily than an LSTM, which has more inductive bias about sequences. 300k samples might be sufficient, but if those are, say, 300k minute bars (~2080 trading hours) for one stock, it’s not huge – a Transformer might memorize noise unless heavily regularized. Unlike language tasks where enormous corpora are available, in finance one often has a limited history. This is why researchers talk about custom, smaller Transformers for time series. Another weakness is the quadratic memory/time scaling with sequence length for vanilla self-attention. If one wanted to attend over 1000 past time steps, the computation could become heavy (though for moderate lengths like 50 or 100 it’s fine). There are solutions (Sparse attention, local attention windows, etc.), but those add design complexity. In practice, using a window of perhaps 30–90 time steps in a Transformer is manageable and probably sufficient for our mid-term signals. Transformers also lack the innate ordering mechanism of RNNs – they rely on positional encoding to inform the model of sequence order. If not done properly, the model might treat data as an unordered set. Usually sinusoidal position embeddings or learned positional vectors are added to inputs to address this. Another subtle weakness: Transformers can struggle with extrapolation. RNNs, by their recurrence, can handle sequence lengths different from what they trained on (to some extent) and naturally produce one-step-ahead outputs in chronological order. A Transformer, if used for direct forecasting, often is set up to predict all outputs in one go (like sequence-to-sequence) or one step with the entire input window given. If we need multi-step forecasting (e.g., will we achieve 3% in the next 5 days?), a direct Transformer output might need to be cleverly structured, or used iteratively with its own outputs fed back, which can degrade performance. There’s active research on Transformers for time-series forecasting to handle such issues – for example, the Informer model uses ProbSparse attention to focus on relevant keys for long series, and the Temporal Fusion Transformer includes gating and static feature enrichment for multi-horizon forecasting. But these enhancements increase model complexity. For a practitioner, another weakness is that Transformers have many tuning knobs: number of layers, heads, embedding dimension, etc., which require careful tuning on validation data. It’s easy to make a Transformer too large and have it overfit or too small and lose its advantage. In one experiment with Chinese stock index data, a plain Transformer slightly underperformed an LSTM in MAE/MSE, possibly due to such optimization difficulties – the authors noted the LSTM was essentially fitting the autocorrelation (a simpler pattern) whereas the Transformer tried to learn more nuanced dependencies
atlantis-press.com
. This hints that Transformers might need more training data or different training regimes (like longer training, different learning rate schedules) to fully realize their edge. Lastly, latency and resource use: although Transformers parallelize well on GPU, if one is running on CPU in real-time, the matrix multiplications for attention could be slower than an LSTM for small batch sizes. In a live trading system, though, this is rarely a deal-breaker since predictions typically happen on the order of seconds or faster, and even a moderately sized Transformer (say 2 layers, 4 heads, 64-dim embeddings) would run in milliseconds on modern hardware. But it’s something to test – ensure that the model can compute a prediction within the available time between data updates. Input Representation: Feeding financial time series into a Transformer requires constructing an appropriate input matrix. Usually, one creates an input sequence of length T (similar to RNN) of feature vectors. Each feature vector (indicators+sentiment at time t) can be linearly projected into a d-dimensional embedding (just as words are embedded in NLP) to feed the Transformer. Alternatively, one can use the raw features as the “embedding” if the first linear layer of the Transformer acts on them. A positional encoding of length T (same length as the sequence) is added to these input representations to encode the order. For example, if we use a 30-day window, we’ll have 30 position encodings added to our 30 input vectors. The model can be an encoder-only Transformer that outputs a representation for the entire sequence (often by pooling or taking the last token) and then uses a dense layer to predict the target (e.g., probability of 3-5% jump). Another approach is a sequence-to-sequence setup where the past window is the encoder input and some future timeline (or a special “forecast” token) is the decoder input; the model then predicts future price or signal as the decoder output. The choice depends on the task – for binary classification (will a 5% move happen?), an encoder that outputs a single classification is sufficient. For predicting a sequence of future returns, an encoder-decoder might be used. The multivariate nature (10 features) is handled by treating each time step’s 10 features as one input token (so the embedding size ≥10, possibly with a linear layer to mix the 10 into a single vector). Some advanced designs include separate encoders for different types of inputs (e.g., an encoder for technical indicators series, one for sentiment series) which then merge through cross-attention. But for simplicity, one encoder handling a concatenated feature vector is common. It’s also worth noting that Transformers can incorporate causal masking if we were generating output step by step; but if we are just doing prediction with a given window, we ensure not to feed any future data (so the input window ends at the last known time before the prediction). Another input representation aspect is scaling and normalization – similar to RNNs, Transformers benefit from normalized data. In fact, Transformers often include layer normalization internally which stabilizes training. Some implementations use time embedding (e.g., encoding the actual timestamp or trading day index) to allow the model to learn seasonal patterns (like year-end rallies or day-of-week effects). For example, an embedding for “Friday” vs “Monday” could be added if those patterns matter for 3-5% weekly moves. In our scenario, adding a simple positional encoding should suffice as most technical indicators already encapsulate some temporal structure. Challenges in Financial Applications: Many challenges Transformers face are shared with RNNs – non-stationarity, overfitting, etc. But a few are notable: Because Transformers can easily incorporate many inputs, there’s a temptation to feed in tons of features, which can actually hurt if those features aren’t truly useful (the model might attend to noise). It’s important to do feature selection or at least be mindful of not overloading the model. The non-stationarity issue can manifest in attention weight patterns – a Transformer trained in one regime may learn to attend to certain patterns that no longer occur later. Regular retraining or adaptation is needed, similar to other models. Overfitting is perhaps an even bigger risk here: with attention, a Transformer could effectively memorize specific sequences of indicators that led to a 5% jump in training data, rather than generalizing the concept. Strong regularization (dropout on attention weights and feed-forward layers, L2 weight decay) and maybe early stopping on a validation set are needed. Transformers might require more data to calibrate, so one challenge is data volume: if 300k samples are not enough, one might augment the data (perhaps include multiple stocks or related series to increase variety) or use transfer learning. There’s some work on pre-training Transformers on large financial datasets (like many years of minute data across stocks) and then fine-tuning on a specific target – akin to how NLP does it. This could be a future direction if Phase 1 shows promise. Another challenge is tuning hyperparameters – unlike LSTMs where a lot of intuition exists (number of layers rarely >2, units 50-200 for many tasks, etc.), with Transformers one might need to experiment with number of heads (more heads could capture different aspects of the data – e.g., one head might focus on volatility pattern, another on sentiment trend), embedding size, and number of layers. Too shallow and it might behave almost like a linear attention weighting of past few days; too deep and it might overfit. One practical pitfall: ensure that the position encoding does not accidentally leak any future knowledge – it usually doesn’t, but if one uses a date-based encoding (like a sinusoid of the absolute date), theoretically the model might infer time distance to some fixed event; better to use relative positioning or standard sinusoidal encoding. Also, care must be taken with evaluation: if we use a Transformer in rolling forecasting, we have to simulate how it will run in real-time (likely feeding last N days and predicting next step repeatedly). This is the same for RNN, but with a Transformer one might be tempted to predict multiple steps at once – which needs careful design to avoid peeking ahead. Finally, interpretability and trust: while we can inspect attention weights, in finance there’s always skepticism whether the model found a real causal pattern or just coincident curves. Using attention visualizations and even attention-based explainers (there are tools like BertViz or Transformers Interpret that have been adapted to time series) can help validate that the model attends to reasonable things (e.g., it focuses on periods of high momentum or big sentiment changes, which intuitively are relevant)
techscience.com
. If it’s attending to seemingly random points, that might indicate overfitting or artifact learning. Empirical Performance: The literature on Transformers for financial time series is nascent but growing. Early results are mixed but often optimistic. Wu et al. (2020) introduced a model for stock movement prediction using Transformers and found it outperformed LSTMs on certain Chinese stock datasets by capturing long-term relations (they noted Transformers can “provide valuable insights” beyond what LSTM learned)
atlantis-press.com
. Another study by Lim et al. (2019) on the Temporal Fusion Transformer (which combines LSTM encoder with multi-head attention for multi-step forecasting) demonstrated SOTA results on several market prediction tasks, with the added benefit of interpretability (identifying important indicators over different horizons). In the A-share stock index study (Lin 2023) we cited, the Transformer slightly underperformed LSTM in raw error, yet was deemed more promising because it didn’t just learn autocorrelation – it discovered some unique dependency structures, implying that given more data or optimization it could surpass LSTM
atlantis-press.com
. There have been examples in crypto markets as well, where Transformers capture the volatile swings better than RNNs when trained on high-frequency data (likely due to the long memory needed for trends in noise). On the other hand, some research (Zhang 2023 review) points out that not all attempts with Transformers succeed – sometimes they end up overfitting small datasets and LSTMs remain superior
ieeexplore.ieee.org
. The key trend is that as datasets grow or as models incorporate techniques to handle limited data (like combining with CNN or using transfer learning), Transformers tend to match or exceed LSTM performance in financial forecasting
atlantis-press.com
scitepress.org
. For example, a hybrid model that uses a 1D CNN to extract local features and then a Transformer to capture global dependencies showed better accuracy than either alone in an intraday stock prediction of Amazon prices
researchgate.net
. Another work combined GANs and Transformer to generate realistic price paths and improve prediction, showing the Transformer’s flexibility in complex setups
arxiv.org
. By 2024, we see Transformers being integrated in competitions and trading systems – often small scale (like 2–4 attention layers) – and they often place among top models. A Scientific Reports article in 2022 by Liu et al. found a Transformer-based model gave the best result on stock trend classification compared to LSTM and CNN (improving accuracy a few percentage points). In summary, Transformers are promising for Phase 1.5 or Phase 2 – they might not be the very first model to deploy due to their complexity, but they are certainly worth exploring once a baseline is established. They may shine especially if the 3–5% events are rare and require piecing together subtle clues from a long history (something attention is well-suited for). Given the user’s data (~300k points, 10 features), a carefully tuned small Transformer could potentially match an LSTM’s performance and even offer easier multi-step forecasting (predicting how far and when the 3–5% might happen). At this stage, however, the safest path is likely to treat Transformers as an experimental upgrade – evaluate them against LSTM on validation data. If the literature is an indication, one might find the Transformer needs more training epochs and careful hyperparameter tuning, but could ultimately identify patterns the LSTM misses.
1D Convolutional Neural Networks (CNNs) for Time-Series
Overview: One-dimensional Convolutional Neural Networks (CNNs) are another deep learning approach adapted from image and signal processing to time-series data. Instead of treating the data as a sequence in time like RNNs, CNNs treat the time axis as a spatial dimension and apply convolutional filters (kernels) across it. A 1D CNN will slide a small filter (for example, a length-3 or length-5 filter) over the time series, detecting local patterns, and then use multiple layers of such filters to build up an understanding of longer-term patterns. CNNs are inherently parallel and do not rely on sequential state, which makes them faster to train and sometimes more robust to noise. In financial forecasting, CNNs have been used on raw price series, technical indicator sequences, and even transformed representations like candlestick images or spectral transforms. A notable example is the “Deep Learning for Order Book” (DeepLOB) model which used CNN to extract features from high-frequency order book data. For mid-horizon forecasting with technical indicators, CNNs can act as feature extractors, capturing patterns like sudden momentum shifts or indicator threshold crossovers that might precede a 3–5% price move. CNNs are also commonly combined with RNNs (CNN-LSTM hybrids) to get the best of both worlds – CNN handles short-term feature extraction, LSTM handles longer-term dependencies. In fact, CNN-LSTM hybrids are the most widely reported successful models in recent literature
techscience.com
techscience.com
, often outperforming either alone. Strengths: CNNs excel at detecting local patterns in the data. For instance, a 1D CNN might easily learn a filter that detects a particular shape in an indicator – say a rapid dip and recovery in the RSI or a crossover of short-term and long-term moving averages – that often signals an upcoming price rally. Because the same filter is applied across the entire time axis, the CNN is translation invariant to time: it doesn’t matter when in the window the pattern occurs, the CNN will recognize it. This is useful because a bullish signal could occur at any point in the recent history before the price moves. CNNs also naturally perform a form of noise reduction by aggregating information across small neighborhoods. A convolution over 5 days, for example, will smooth out day-to-day noise while picking up consistent directional moves. Another strength is efficiency and parallelism: CNN computations are vectorized and can be run very fast on modern hardware. They don’t carry a hidden state over time, so they avoid the sequential bottleneck of RNN training. This means CNNs can handle longer input windows more easily in training – e.g., a CNN can look at a 100-day window with only modest increase in computation, whereas an LSTM’s training time would grow linearly with sequence length. CNNs also tend to have fewer parameters than an equivalent fully-connected network, because weights are shared across time positions. This reduces overfitting risk to some extent. In comparisons, CNNs have outperformed MLPs on sequence data by virtue of this weight sharing and local receptive field. For example, one study showed a simple CNN achieved lower prediction error than a deep MLP for weekly stock price forecasting, attributing it to CNN’s ability to extract temporal features that the MLP couldn’t
gavinguan95.github.io
. CNNs can also incorporate multivariate inputs effectively by having multiple input channels: each technical indicator can be one channel, so a filter spans all features over a few time steps. This way, a convolution filter could learn cross-feature interactions (e.g., “MACD is rising and volume is surging simultaneously over 3 days”) that are indicative of a price change. Additionally, variations of CNN like Temporal Convolutional Networks (TCN) introduce dilated convolutions and causal padding to increase the receptive field and ensure no leakage of future data. TCNs have been shown to handle long sequences and have memory comparable to RNNs by increasing dilation (skipping some steps in convolution)
techscience.com
. Such architectures can capture long-term dependencies by using progressively dilated filters while still benefiting from convolutional efficiency. Another practical strength: CNNs are easy to configure and train – one usually starts with a few convolutional layers and maybe a pooling layer, and it either works or one can adjust filter sizes and counts; there are fewer tricky hyperparameters than with LSTMs (no need to worry about gradient clipping or hidden state sizes as much). Weaknesses: The main limitation of CNNs is their fixed receptive field. A CNN sees at most K time steps of information by design in a convolution layer (where K is the kernel size, or larger if multiple layers are stacked). If an important signal lies outside that window, a plain CNN won’t catch it unless you stack enough layers or enlarge the kernel. For instance, a CNN with two convolutional layers of kernel size 3 has a receptive field of 5 (roughly). To cover 20 days of context, you might need many layers or kernels of size covering that span, which can increase complexity. While stacking layers extends the effective receptive field, very deep CNNs can become harder to train (though techniques like residual connections can help). Without modifications, CNNs also do not inherently remember long-term past beyond the window – if the pattern that started 30 days ago is relevant, a CNN must either have had that in its input window or else it has no internal state to recall it. In contrast, an RNN could carry a hint of something it saw 30 days ago in its hidden state. Another weakness is that CNNs treat all positions within their convolution equally except at the boundaries where padding occurs. Financial time series often have time-varying volatility or regime where the significance of a pattern might depend on when it occurs (e.g., a momentum burst near a major support level vs. in the middle of a trend). A CNN won’t inherently know “this pattern happened after a long flat period” unless that context is also within the convolution window. So CNNs might need to be combined with some trend indicator input to contextualize patterns. Additionally, if the data has features on very different time scales (say a 5-day cycle and a 50-day cycle), a single CNN filter might not capture both – you’d need multiple filters or multi-scale filters. Some approaches use multiple kernel sizes or wavelet transforms to capture multi-scale patterns. This is a bit more complex to design compared to an LSTM which might learn a short and long-term memory naturally. Another potential issue: Many CNN architectures use pooling layers (downsampling) to reduce dimensionality. In time series forecasting, pooling must be used cautiously – if you downsample the time axis, you lose some temporal resolution that might be needed for precise event timing. Some models avoid pooling and just use stride-1 convolutions or dilations to keep resolution. Without pooling, however, very deep CNNs can become computationally heavy or prone to slight overfitting at the edges (due to many adjacent correlated features). In practice, pooling is often skipped for forecasting (unlike image CNNs) to preserve timing. A more subtle weakness is that CNNs assume stationarity within the window to some degree – a filter learned on one part of the series is applied to another. If the statistical properties shift even within the window, the filter’s applicability might reduce. Lastly, lack of an explicit memory: while CNN can emulate some memory by deep layers, it doesn’t have a mechanism like gating to decide to keep or forget information. If something important happened 15 bars ago, it’s either captured in the convolution outputs up to now or it’s gone. This is why CNNs are often paired with LSTMs: CNN extracts features over short windows, then LSTM carries those features forward in time. Without that, a pure CNN might need a very large kernel or many layers to match the sequence retention of an RNN. Optimal Input Representation: Using a CNN for time series is relatively straightforward: one typically constructs an input matrix of shape (number of time steps, number of features) for the lookback window. For example, a 30-day window of 10 features can be seen as a 10×30 “image” (with 10 channels and 30 width, or equivalently 30 “time pixels” with 10-dimensional value each). Convolutional filters will span across the time dimension and (usually) fully across the feature dimension if we treat features as channels. For instance, a filter might be 5 days long and cover all 10 features – this allows it to detect a pattern involving all those features in any 5-day segment. Some implementations treat each feature as separate channels initially and use 1D convolutions that convolve across time independently per feature, then later merge. But more common is to convolve across all features at once, so the CNN can learn cross-feature interactions. The stride is typically 1 (the filter moves day by day), ensuring no days are skipped in pattern detection. We add zero-padding at the edges so that the output of convolution remains the same length as input (important if we want to align outputs to predictions). The CNN might have multiple filters (say 32 or 64) in the first layer to capture different types of patterns (one might focus on momentum oscillations, another on volatility spikes, etc.). Stacking layers means the second layer’s filters operate on a representation that is already a pooled summary of a few days. By the second or third layer, the receptive field can cover, e.g., 7-10 days even if each layer’s kernel is small, so deeper layers start to capture higher-level patterns (like “a rise-followed-by-dip” pattern or “a trend followed by a consolidation”). As with other networks, normalization of inputs helps CNNs too (though they are a bit less sensitive than RNNs to scaling). If different indicators have very different scales, one may normalize each channel (feature) to a comparable range so that the convolution weights don’t have to compensate for scale differences. Another input trick is to augment channels with technical indicators that highlight specific patterns. For example, one could feed not just raw price or returns as features, but also a binary feature that marks where a certain condition is true (e.g., “1 if RSI > 70, else 0”), which a convolution can then easily pick up as a distinctive bit pattern. However, if those technical conditions are already reflected in indicators themselves, it may be redundant. Some researchers also transform time series into other domains for CNNs – like generating a time-frequency representation (spectrogram) or encoding recent price moves as an image – but in our scenario, since we already have meaningful indicators as features, a direct time-domain CNN is appropriate. A CNN can also be applied in a rolling fashion: one can slide the CNN window over the series and produce an output at each time (like a rolling prediction). But more often, one uses the CNN to analyze the fixed window and output a prediction for, say, the next step or a classification. If doing multi-step forecasts, one might use an encoder CNN on past data and perhaps a decoder CNN (or simpler method) to generate multiple future points, but that’s less common than either predicting one step or using sequence-to-sequence CNN (which is a bit exotic). Challenges in Financial Applications: Many challenges for CNNs overlap with those of other deep models. Non-stationarity: if the nature of patterns changes, the CNN’s fixed filters might become less effective. For instance, a filter detecting a certain candlestick formation might work in a bullish market phase but not in a choppy market. The CNN might then trigger false signals or miss new patterns. Retraining periodically or using adaptive filters (perhaps via transfer learning) is a way to handle this. Selecting the right window size and filter sizes is a non-trivial challenge – it requires experimentation or domain knowledge. If the window is too short, CNN sees too little context; if too long, the benefits diminish and it might include regime shifts within one input. Often window sizes are chosen by cross-validation. A related challenge is multiple time-scale patterns: a CNN of fixed architecture might not simultaneously capture very short-term and moderately long-term patterns well. Solutions include using multiple parallel convolution paths with different kernel lengths (like an Inception module for time series) or using dilation in convolution to cover longer gaps. Some studies have used wavelet transforms feeding into CNNs to capture multi-resolution patterns of financial data. Another issue is data sufficiency: CNNs, like any deep model, need enough examples of the patterns to learn them reliably. If 3–5% moves are infrequent, the CNN must be trained in a way that it actually sees enough occurrences (for instance, by oversampling segments that lead to such moves or by data augmentation such as adding noise to create variant scenarios). Overfitting is a risk if the CNN has many filters – it might memorize particular sequences of indicator values as “patterns” that aren’t generally predictive. Techniques like early stopping and dropout (applicable to CNN as well) help. Dropout can be applied to convolutional feature maps to prevent reliance on specific activations. Another challenge is that CNN outputs can sometimes be hard to interpret (even though one can visualize filters). However, one might inspect which patterns the filters are detecting by feeding various inputs and seeing what excites a filter. In finance, some have visualized first-layer filters to find they correspond to, say, a spike detector or an upward trend detector. Ensuring causality is also critical: one must avoid using future data in convolution inadvertently. This means if we use padding, it should be done carefully (usually we use padding on the left side of the sequence for causality, not symmetric padding, so that we don’t include future info – or ensure the CNN is only used to predict after the filter has fully seen past data). Many implementations use “causal convolution” where the filter is shifted such that it only covers past and present, not future. This is important if we want the CNN to output something aligned with the last input time. If we are sliding the CNN and making predictions in real-time, we’d want a causal setup. If we are just doing one-shot prediction with the entire window, just be sure not to include any data beyond the point of prediction. Class imbalance or objective mismatch can be a challenge if we are looking for an event (3–5% move). A CNN trained on regression of returns might minimize MSE by focusing on the bulk of small returns, not specifically learning to anticipate the bigger moves. In such case, one might train the CNN for classification (e.g., will a 3% move happen in next 10 days: yes/no) to directly target the event of interest. This could involve balancing the training data. Empirical Performance: CNNs by themselves have shown competitive results in various financial forecasting tasks, though often they shine best in hybrid configurations. For example, Gunduz et al. (2017) transformed stock data into 2D images (by plotting indicators on a 2D grid) and used a CNN to classify buy/sell signals, achieving high accuracy – showing CNN’s pattern recognition prowess. In pure 1D usage, Zhang et al. (2019) found that a 1D CNN could outperform an LSTM in forecasting certain stock prices when the task was to predict short-term direction, likely because the CNN picked up quick technical patterns whereas the LSTM might have over-complicated the pattern
gavinguan95.github.io
. In the comparative study by Duong et al. (2022) we saw earlier, CNN was the top model for one of the stocks (UNH) when using a 1-week input window
annals-csis.org
, beating RNN, LSTM, and others for that particular case. The authors observed that “the CNN model is the best model in this case”
annals-csis.org
, illustrating that CNNs can sometimes capture the price formation in a short horizon better than recurrent models. However, their overall conclusion still favored RNNs on average for very short-term, and suggested CNN might do well in certain sectors (they noticed CNN was best for stocks in Healthcare sector in their experiments, whereas RNN was best in others)
annals-csis.org
. This hints that CNNs might be more suited for some types of data patterns (perhaps more regular or mean-reverting ones). The most striking results for CNNs are in hybrid models: numerous papers report that a CNN+LSTM combination outperforms either alone
onlinelibrary.wiley.com
. For instance, Lu et al. (2020) showed that a CNN-LSTM ensemble provided the highest prediction accuracy for stock prices compared to standalone CNN or LSTM
onlinelibrary.wiley.com
. The CNN would first extract short-term features (like price wavelet or recent trend) and the LSTM would then model the temporal evolution of those features, capturing the longer context – yielding superior results. Similarly, Qin et al. (2017) used CNN to extract features from multiple technical indicators and then a GRU with attention to forecast stock trends, achieving notable improvements over pure GRU. Another interesting use is in financial time-series classification (like detecting a particular pattern or regime); CNNs often achieve high accuracy due to pattern-matching ability. For predicting mid-horizon movements (3–5% returns), one might find that CNNs can effectively flag the early signatures of such moves (like breakouts or momentum bursts). Empirical evidence suggests that trend-following indicators and momentum oscillations are well captured by CNN filters, which is why adding those indicators as channels can boost CNN performance. In Kaggle competitions or academic challenges where participants tried various models for stock prediction, CNNs usually rank near the top, though rarely the outright winner unless combined with others. A safe characterization is that CNNs provide complementary strength: they excel at short-term feature extraction and denoising, but need help (from either additional layers or another model) to handle longer-term dependencies. In our implementation context, a 1D CNN could be used to preprocess inputs for an LSTM (essentially acting as an automated feature engineer). Alternatively, one could start with a CNN model alone as a baseline to see if it catches obvious patterns. The computational efficiency of CNNs also means one could run an ensemble of multiple CNN models (with different architectures or indicator subsets) in parallel and combine their predictions with very little latency hit – which is appealing in a real-time system. Notably, a recent study by Saud & Shakya (2024) (cited earlier) found that combining multiple models is beneficial
oaji.net
, and they specifically saw that a simple combination of indicators into one model was inferior to splitting and ensembling
oaji.net
. This could be interpreted as CNNs, each tuned to specific indicator patterns, might collectively do better than one model trying to learn everything. While they used RNNs in that ensemble, one could conceive a similar ensemble of CNNs (one specializing in trend features, one in momentum, etc.). Overall, CNNs have proven effective for financial forecasting, especially when the goal is to detect what pattern precedes a movement. For Phase 1, one might use a CNN either as a standalone benchmark or more promisingly as part of a hybrid (CNN+LSTM) model which, according to extensive literature, tends to yield state-of-the-art results
techscience.com
techscience.com
.
Key Technical Indicators and Feature Engineering for Each Architecture
Using the right technical indicators as input features is critical for any model to predict mid-horizon trading signals. Technical indicators encapsulate domain knowledge about market behavior (trends, momentum, volatility, volume, etc.) in a form that models can more easily ingest than raw price series. In this use case, the user already has 9 technical indicators and 1 sentiment score. We will discuss which types of indicators are generally most informative for forecasting ~3–5% returns, and how each architecture might leverage them:
Trend-Following Indicators: These include moving averages (MA) of different lengths, Moving Average Convergence Divergence (MACD), and Average Directional Index (ADX) among others. They help quantify the direction and strength of the prevailing trend. For predicting a 3–5% move, trend indicators are often crucial – a strong existing uptrend (high ADX, price above long MA) can imply continuation, whereas a divergence or crossover (e.g., MACD line crossing signal line) might indicate an upcoming reversal or acceleration. In an MLP, trend features are essential because the MLP has no memory; giving it, say, the 20-day and 50-day moving average values (and their crossover status) at the current time allows it to infer trend direction. An RNN/LSTM could, in theory, learn the trend from raw prices, but feeding it an indicator like MACD simplifies the task – indeed, researchers often include MACD or moving averages as inputs to LSTMs and find improved performance
researchgate.net
researchgate.net
. The LSTM can then focus on interactions and deviations (e.g., maybe the distance between price and MA). CNNs can use trend indicators as separate channels and may learn filters like “if price is consistently above MA and rising” which is a trend-following pattern. A CNN might also catch patterns in the ADX value (e.g., a rising ADX indicates strengthening trend). Transformers can attend to trend indicator trajectories over time – for instance, the attention might highlight when a short-term MA crossed above a long-term MA a few days ago, contributing to the likelihood of a sustained upward move. In summary, trend indicators (like SMA, EMA of various periods, MACD histogram, ADX) are generally highly useful for all architectures. They act as a compressed representation of market direction and can help models distinguish between trending vs. range-bound conditions. Empirically, trend indicators often appear among the top features in feature importance analyses for stock prediction
mdpi.com
. For example, a study using XGBoost and LSTM found that moving averages and MACD were key predictors for stock returns
doaj.org
.
Momentum Oscillators: These include Relative Strength Index (RSI), Stochastic Oscillator (%K, %D), Commodity Channel Index (CCI), Momentum indicator (rate of change), and others like Know Sure Thing (KST). These oscillators measure the speed and magnitude of price movements and often oscillate between fixed bounds, indicating overbought or oversold conditions. For forecasting mid-term moves, momentum indicators are valuable because a reading of extreme overbought (e.g., RSI > 70) might signal an impending correction (price might drop or pause before another rise), whereas an oversold condition (RSI < 30) might signal an upcoming rebound (potentially yielding a 3–5% bounce). MLPs benefit from having momentum indicators because they summarize recent price action dynamics – an MLP neuron could learn that “if RSI crossed from below 30 to above 30, a short-term rally is likely.” Without RSI, the MLP would have to deduce that from raw returns, which is harder. LSTMs/GRUs can learn momentum shifts by looking at sequences of returns, but providing an oscillator (which is essentially a smoothed and bounded view of momentum) can guide the LSTM’s state. Indeed, many LSTM models for stock trend include RSI as an input
ieeexplore.ieee.org
. GRUs can also integrate momentum easily; an ensemble study found KST oscillator was the most proficient trend indicator among MACD, DMI, and KST when used with GRU/LSTM models
oaji.net
oaji.net
, highlighting that complex momentum indicators can carry a lot of predictive signal. CNNs are quite adept at picking up oscillator patterns – for example, a CNN filter could detect a “bullish divergence” (price making lower lows while RSI making higher lows) if RSI is one channel and price or another indicator is another channel. Such divergences often precede trend changes and a CNN can spatially detect that pattern in a window. CNNs can also detect specific oscillator thresholds being crossed or the shape of an oscillator curve (like a double dip in RSI). Transformers could attend to times when momentum oscillators hit extreme values or changed direction sharply, as these points might be critical (the attention mechanism might assign more weight to times where RSI was at 20 and rising, for instance, which could foreshadow a big upward price move). Overall, momentum indicators are vital features across architectures. They often rank as important predictors because they quantify short-term sentiment and exhaustion. For example, RSI and Stochastics were used in an LSTM model by Fischer (2018) to enhance prediction of short-term stock swings, and KST was highlighted in 2024 as particularly effective for anticipating price moves
oaji.net
.
Volatility Indicators: A common one is Average True Range (ATR) which measures average range of price movement, and Bollinger Bands (which derive from volatility). Also, metrics like historical volatility (std dev) or Bollinger Bandwidth can be used. These indicators signal whether the market is in a low volatility consolidation or high volatility state. For a 3–5% return event, volatility can play two ways: a low-volatility phase often precedes a larger breakout move (coiled spring effect), and conversely an extremely high volatility might occur during a big move (and possibly mark a climax). Including volatility measures can help the model modulate its predictions. MLPs can directly take ATR or Bollinger band width as input to gauge whether the current volatility regime is conducive to a breakout. For instance, an MLP neuron might learn that “if volatility has been very low (narrow Bollinger Bands) and other conditions are right, expect a larger move soon.” LSTMs can incorporate volatility in their state – e.g., the LSTM might use an elevated ATR reading to inform that recent moves have been large, which could either mean momentum or risk of reversal. LSTMs might also benefit from volatility-normalized inputs (some models feed returns and also volatility so the LSTM can consider risk-adjusted returns). CNNs could have filters that react differently under different volatility regimes if ATR is provided as a channel. They might detect patterns like “a volatility contraction followed by a price breakout.” Indeed, pattern recognition like squeeze (low volatility, then expansion) is something a CNN can capture if given Bollinger Bandwidth or similar. Transformers could attend to when volatility changed – e.g., if volatility has been steadily rising, the model might attend to that as a sign of a trend strengthening (or chaos). Also, attention could catch instances from the past where similar volatility levels preceded a move. Empirical evidence: volatility indicators are often part of successful trading models. For example, a study by Zhong and Enke (2019) included ATR and Bollinger %B as inputs to a neural network and found them helpful in improving accuracy of stock direction forecasts. ATR specifically appears in many algorithmic strategies to set stops or targets; as a feature, it can help a model know if a 5% move is within normal range or a significant outlier at the moment. In summary, volatility features help gauge the context (quiet vs volatile market), which can influence the probability of hitting a 5% return threshold. All architectures can make use of this contextual info to moderate their predictions.
Volume-Based Indicators: Volume is often an early indicator of interest in a move. Indicators like On-Balance Volume (OBV), Chaikin Money Flow, Volume Rate of Change, or simpler metrics like volume moving averages, can be included. A rising price on increasing volume is more convincing (higher chance of continuation) than on declining volume. For mid-term signals, volume indicators can confirm moves or divergences (e.g., price rising but OBV flat or falling might indicate a weak rally likely to fail). MLP can directly use OBV or a volume spike flag as a feature to understand if the recent price action was supported by strong trading activity. RNNs can benefit from volume sequences – e.g., an LSTM might learn patterns like accumulation (volume increasing while price is flat, which often precedes an uptrend). Including volume indicators explicitly (like OBV, which accumulates volume on up days minus down days) provides a distilled view of smart money flow that the LSTM can incorporate. CNNs with volume channels can identify patterns such as volume climaxes (very high volume days often at turning points) or consistent volume upticks. For instance, a filter might detect “price up bar accompanied by huge volume” – possibly a breakout signal. Transformers could attend to periods of unusual volume as key points. There’s evidence in literature that volume-based features improve predictive performance: e.g., a 2021 study by Rather et al. found that adding volume indicators to an LSTM improved stock trend classification accuracy compared to using price-based indicators alone. Also, volume indicators like Chaikin Oscillator (which was listed in the earlier table of indicators
journalofbigdata.springeropen.com
) combine price and volume information – models can latch onto those combined signals. In practice, volume often differentiates false breakouts from real ones; thus, any model, whether MLP or complex, does well to consider it. If the user’s 9 technical indicators did not include a volume metric, it might be wise to add one.
Sentiment Indicator: The user has a sentiment score, which is an external feature likely derived from news or social media analysis. This kind of feature can capture information not present in price/technical data – for example, extremely positive news sentiment could catalyze a 5% rally even if technicals were neutral, and extremely negative sentiment might trigger a sell-off. Incorporating sentiment into models has been shown to enhance performance
nature.com
. In our context, MLPs can treat sentiment as just another input; one might find that a high sentiment score combined with certain technical patterns yields a higher probability of an up move. RNNs/Transformers might treat sentiment as a time series of its own – a Transformer could even be structured to jointly attend to past price patterns and past sentiment patterns. If sentiment tends to lead price (e.g., news shifts before price does), the model can learn that relationship. GRUs/LSTMs could especially benefit if sentiment has a longer effect lag – the sentiment can be fed in at each time step and the recurrent network might carry forward its influence. There’s a recent example: H. Lee et al. (2024) combined an ESG sentiment index with technical indicators in a deep model for S&P 500 prediction and found improved accuracy when considering sentiment vs. using only technicals
nature.com
nature.com
. This confirms that sentiment data can provide an edge. Each architecture can integrate this: an LSTM could learn that “if sentiment has been steadily rising the last few days, and technicals are aligning, the likelihood of a breakout is higher.” A Transformer might explicitly attend to the moment sentiment spiked as an important reference for the prediction. One important aspect is scaling/processing sentiment: sentiment scores might be bounded (e.g., -1 to 1) or unbounded depending on how they’re computed; they should be normalized if needed and perhaps smoothed (news sentiment can be noisy day-to-day, some take a moving average). But in any case, sentiment is a feature that adds a fundamental dimension to predominantly technical models, potentially catching catalysts that pure price-based indicators miss.
It’s worth noting that the effectiveness of each indicator can depend on the model. For example, a simple MLP might need an indicator like MACD to “know” there is a trend change, whereas an LSTM might detect the trend change by itself from raw prices – yet even for the LSTM, having MACD could shorten training or improve stability. Researchers have explored feature importance for different models: one study found that tree-based models and LSTM both prioritized moving averages and volatility indicators, but LSTM also made use of momentum indicators strongly
researchgate.net
. This suggests deep models do utilize these technical features in slightly different ways, but universally some indicators (moving averages, RSI, etc.) are useful. There is also evidence that combining multiple types of indicators (trend + momentum + volume) is better than using any single category alone
sciencedirect.com
 – they provide complementary information. However, as seen in the ensemble RNN paper
oaji.net
, simply throwing all indicators into one model can sometimes be less effective than expected (possibly due to multicollinearity or the model getting confused by too many inputs). That study discovered using separate models for each indicator category and then ensembling gave better results, and also that among trend indicators tested, KST (a complex momentum indicator) was the most predictive
oaji.net
. This highlights that indicator selection matters. In a practical sense, one should feed a diverse but not redundant set of indicators. The user’s 9 indicators likely cover various aspects; if not, one might consider adding those missing (e.g., if no volume indicator is present, consider adding one). For each architecture:
MLP: Highly reliant on indicator quality. Best to feed it a rich set of indicators that encapsulate short-term and medium-term signals (e.g., a 5-day momentum, a 20-day trend, a volatility measure, etc.). Scaling each indicator to a similar range is important for the MLP. MLP might perform feature selection implicitly (via weights), but it can be helped with some pruning of useless features. Because MLP can’t learn time relationships beyond input vector, one might include indicators that themselves have some time memory (like a 14-day RSI or a 10-day OBV change). Those effectively inject some temporal knowledge. In short, MLP needs informative snapshots: indicators that are computed over appropriate lookback periods to capture what’s needed for a 3–5% forecast.
RNN (LSTM/GRU): These can utilize raw indicators over time. An LSTM could even be fed just price, and it could internally compute something akin to an MA or RSI. But providing indicators accelerates learning. LSTMs often show better convergence when indicators are inputs because it reduces the burden on the network to calculate complex functions of the sequence; instead it can focus on the sequence of those higher-level features. For example, feeding both closing price and a 10-day SMA allows the LSTM to easily see their interaction (like a crossover) without having to implicitly calculate the average itself. Bidirectional LSTMs used in research (not for live) have even taken entire indicator sequences and found patterns in them that correlate with past known outcomes
ijsrd.com
. GRUs similarly will use whatever features to update their gated state. One potential difference: GRUs/LSTMs might handle slight redundancies in features better than MLP (since they can weight them over time), but one should still avoid overly correlated indicators that just add noise (e.g., feeding both 10-day and 15-day MA might be okay, but 10-day and 11-day MA probably unnecessary).
Transformer: It can handle many features too, but since it doesn’t have an innate bias for continuity like RNN, having clear indicator signals helps it pinpoint important moments. For instance, if using attention, the model might latch onto moments where an indicator triggered a known condition (like “sentiment jumped” or “volatility regime changed”). The “Temporal Fusion Transformer” actually includes a mechanism to learn the importance of each input feature over time; it often finds that technical indicators like volatility or volume have seasonally varying importance. For a plain Transformer, we might consider adding indicator encodings (like categorical embeddings if an indicator is above/below certain threshold) but that’s advanced. Usually just feeding the numeric indicators is sufficient. In some attention-based models, technical indicators have been used to bias the attention – e.g., an attention head that focuses on volatility events. However, that's beyond a baseline. In general, a Transformer will benefit from the same set of indicators as an RNN would.
CNN: Since CNNs look at patterns in a window, having multiple indicators means the CNN can detect multi-indicator patterns (which is akin to what a technical analyst might do visually on a chart: e.g., see price forming a certain pattern while RSI confirms momentum and volume confirms participation). For example, a classic bullish signal might be: price breaks above a moving average and RSI crosses above 50 and volume is higher than average. A CNN could have filters that respond when all these conditions in a short span are met – essentially learning a compound pattern. If only raw price was given, the CNN might not catch that complexity as easily. So providing these additional channels (MA, RSI, volume) explicitly enables CNN to act like a pattern recognizer that a human trader would do with multiple indicator overlays. A study by Jiang et al. (2020) applied CNN to multiple technical indicator series and found the filters could indeed pick out known technical patterns from them, improving prediction of stock trends versus using price alone.
Most Effective Indicators (Summary): From the above categories, commonly the most effective indicators for mid-horizon (swing trade) predictions are:
Moving Averages (5-day, 20-day, etc.) and MACD: for trend direction and crossover signals.
RSI (14-day is typical) or Stochastic Oscillator: for momentum and mean-reversion signals.
ATR or Bollinger Bands: for volatility regime awareness.
Volume metrics (OBV, volume MA): for confirming moves.
Sentiment index: for capturing external drivers.
These, in combination, cover different facets of the market and have been found to improve model accuracy
nature.com
sciencedirect.com
. Notably, one should align the indicator periods with the forecast horizon – e.g., 14-day RSI is reasonable for predicting moves that might unfold over days to a couple weeks; a 200-day MA might be too slow for a 3–5% swing (unless the question is about a very large swing), whereas a 50-day MA could be relevant to the general trend context.
Finally, feature engineering should consider interactions: Some models explicitly create features like the ratio of two indicators or difference between them (e.g., %K – %D in Stochastics, or price – MA as a percentage). These can also be fed as additional inputs. The advantage is giving the model a simpler input to represent something like “distance from moving average” which a linear model or MLP can use easily. Deep models can learn those interactions themselves, but providing them can speed up training or make the model smaller. For example, the MACD itself is basically difference of two EMAs; by feeding MACD, we spare the model from computing that difference from two separate EMA inputs. In conclusion, each architecture benefits from a thoughtful set of technical indicators that capture trend, momentum, volatility, volume, and sentiment:
MLP absolutely requires them to have any notion of time-derived features.
RNN/CNN benefit by having to learn less from scratch and focusing on higher-level patterns.
Transformers can leverage them to focus attention on meaningful events.
Empirical studies consistently show improved performance when combining technical indicators with deep learning models
nature.com
, confirming that these hand-crafted features still provide significant value even in end-to-end learning setups. The key is to avoid information overlap too much (to reduce noise) and to scale all features properly so that, for instance, an RSI (0–100 range) and a price (which could be in tens or hundreds) don’t have disproportionate influence simply due to scale. In our scenario, the provided 9 indicators likely span these categories, and the sentiment score adds a unique dimension – together, they should give any of the architectures a rich signal substrate to learn from.
Comparative Performance and Recommendations
Bringing it all together, we compare the architectures in the context of the user’s mid-horizon (3–5% return) forecasting problem. Below is a comparative summary table highlighting suitability for the given dataset, along with notes on performance and usage:
Model	Typical Performance in Literature	Notes for 3–5% Signal Prediction
MLP (Baseline)	Often underperforms sequence models on financial data
annals-csis.org
annals-csis.org
; improved by good feature engineering.	Use as a benchmark. Requires well-chosen lagged features (indicators) to have any predictive power. Fast and low-resource, but likely lowest accuracy.
LSTM/GRU (RNN)	Strong track record, state-of-the-art in many studies
techscience.com
techscience.com
. Captures temporal patterns; outperforms MLP/RF in most comparisons.	Recommended as Phase 1 baseline model. Handles multivariate input and medium-size data well. GRU is a lighter alternative to LSTM if faster training is needed. Use dropout to prevent overfitting.
Bidirectional RNN	Yields higher accuracy in research (uses future data)
ijsrd.com
, but not usable for real-time forecasting (needs future context).	Not applicable for live trading (can't peek future). Could be used offline to assess pattern recognition performance or as part of an ensemble for confirmation signals.
Transformer	Emerging results show equal or better performance than LSTM in some cases
atlantis-press.com
dl.acm.org
, but can be data-hungry. Mixed outcomes if data is limited.	Promising but secondary choice. Try a small Transformer after establishing LSTM baseline. Could capture complex relations (e.g., sentiment-price interactions) and long-term context. Ensure adequate regularization and tune on validation set.
1D CNN	Good at short-term prediction; has outperformed LSTM in certain short-horizon tests
annals-csis.org
. Generally improved when combined with LSTM or attention
onlinelibrary.wiley.com
.	Useful for feature extraction. On its own, may give decent results if 3–5% moves have distinct local patterns. Ideally, use in hybrid (CNN+LSTM) to boost performance. Very fast inference – suitable if low latency is crucial.

In terms of latency and feasibility on a local system: all these models can run quickly for a single prediction. An MLP or small CNN will be virtually instantaneous. An LSTM with a few hundred units or a 2-layer Transformer with, say, 4 heads and 64-dim embeddings will still run in milliseconds on a CPU for one input sequence. Given we are not in high-frequency regime, latency is not a major limiting factor; even the more complex models should be fine for making real-time decisions on, say, minute or hourly bars. If using an ensemble or many models, then efficiency might become a consideration, but one can always start with a single model. Most Effective Architecture for the Task: Based on the literature and the dataset characteristics, Recurrent neural networks (especially LSTM or GRU) are the most reliable choice for Phase 1. They directly address the time-series nature of the problem and have proven performance on similar tasks. LSTMs can leverage the 10 input features over hundreds of thousands of samples and find temporal patterns leading to 3–5% moves, while being robust to moderate dataset sizes. In contrast, an MLP would likely ignore sequence effects, and a Transformer, while potentially powerful, might overfit or require more tuning effort. A CNN alone might miss long-term context that sometimes matters for a sustained move (e.g., a base breakout that builds over weeks). However, the best performing models in literature are often hybrids – notably CNN-LSTM or LSTM with Attention. Chen et al. (2024) conclude that “hybrid models like CNN-LSTM-AM have proven superior to stand-alone models” in financial time-series prediction
techscience.com
. Specifically, a CNN+LSTM combines the CNN’s pattern extraction with LSTM’s sequence memory, and adding an Attention Mechanism (AM) on top helps with very long-term dependencies. These hybrids have achieved state-of-the-art results, outperforming either CNN or LSTM alone
techscience.com
. For instance, a CNN-LSTM model was found to give the highest accuracy in stock price forecasting compared to CNN-only or LSTM-only in one study
onlinelibrary.wiley.com
. Given this evidence, one might aim for a hybrid architecture eventually. But for an initial implementation (Phase 1 baseline), introducing a hybrid adds complexity. It could be wise to start with an LSTM (perhaps 1 or 2 layers) and then experiment by adding a preceding CNN layer or an attention layer to see if it improves validation performance. Architecture Ranking (for Phase 1 Implementation):
LSTM (or GRU) – Top Choice: It directly suits the problem and has a high chance of success. For example, an LSTM with 50–100 units, fed with sequences of the past N periods of 10 features, can be trained to output a trading signal. This model leverages proven capability of LSTMs to handle multivariate financial data
techscience.com
. GRU is a fine alternative if a slightly simpler model is preferred (it will train faster with potentially similar results). Ensure to use techniques like dropout between LSTM layers (or on recurrent connections) to generalize better
techscience.com
. Monitor for overfitting by checking performance on a validation split or using walk-forward validation.
CNN-LSTM Hybrid – Strong Candidate: If resources and time permit, consider this as an extension of the LSTM baseline. For example, apply one 1D convolution layer (with, say, 16 filters of size 3 or 5) to the input sequence to extract high-level features, then feed the output sequence of that convolution into an LSTM layer. The CNN will act on the technical indicators over short time spans (denoising and highlighting patterns), and the LSTM will track these patterns over longer spans. Literature suggests this approach can yield better accuracy than LSTM alone
techscience.com
. It might be slightly more complex to tune (two types of layers instead of one), but it’s quite feasible with Keras/PyTorch libraries. For Phase 1, one could implement this after first getting an LSTM working, as a next step to push performance.
Transformer (Small) – Experimental: A lightweight Transformer model (for instance, 2 encoder layers, 2-4 attention heads, sequence length maybe 30-60) could be tried to see if it captures any additional signals. It may or may not immediately outperform LSTM on this mid-frequency task. The A-share study indicated Transformers can learn unique dependencies, which might become advantageous
atlantis-press.com
. So, if the LSTM is doing well but you suspect there are subtle patterns (perhaps involving long gaps or interactions that LSTM might miss), a Transformer is worth a shot. Use the same indicators as inputs, and compare results. This would be more of a Phase 2 exploration or a benchmark to see if the latest architecture offers improvement. Keep in mind to avoid an overly large Transformer to prevent overfitting on ~300k points.
1D CNN (Standalone or Ensemble): On its own, I would rank CNN below LSTM and Transformer for this particular problem because of the sequence length and mid-horizon nature. A CNN might excel if the target pattern was very localized (like predicting a next-day jump purely from a certain candlestick formation in the last week). But a 3–5% move might sometimes require noticing a pattern that develops over a couple of weeks – pure CNN would need to be deep to cover that. Still, a CNN could be used as a quick model to see if it catches obvious signals. Another use is to create an ensemble: e.g., have a CNN model and an LSTM model and combine their predictions (averaging or voting). Since CNN and LSTM have complementary strengths, an ensemble can improve robustness. The ensemble idea is supported by research which found that combining models (especially those focusing on different indicators) improved prediction accuracy and stability
oaji.net
oaji.net
. For Phase 1, an ensemble might be beyond scope, but it's a possible enhancement later. If including a CNN, ensure it uses causal convolution if needed and consider using dilations if you want it to cover longer history without too many layers.
MLP (Benchmark): The MLP is mainly to have a baseline to compare against. In practice, it’s unlikely to beat the others, but it’s useful to quantify how much better the sequence models are. One might implement a simple MLP that takes as input the last, say, 5 or 10 days of features (flattened) and predicts the outcome. If the LSTM/CNN/Transformer significantly outperform the MLP, it validates that their sequence learning is adding value beyond what static features provide. On the other hand, if MLP is close in performance, it might indicate the indicators already encapsulate most info and the sequence aspect is less crucial (which would be surprising but not impossible if, for instance, the indicators themselves are forward-looking or the sentiment is very telling).
Given the user’s goal of an autonomous real-time trading system for mid-horizon signals, my recommendation for Phase 1 is to implement a supervised LSTM-based neural network as the primary model. Start with a unidirectional LSTM (e.g., one or two layers) using the multivariate input of technical indicators and sentiment. Use supervised labels such as a classification of whether a 3–5% upward (or downward) move occurs within the forecast horizon, or a regression of expected return which can be interpreted into a signal. Train this model on historical data, using a rolling-origin evaluation or cross-validation to ensure it generalizes to unseen periods (important due to non-stationarity). Monitor performance metrics relevant to trading (like precision/recall for signal events, or profit metrics in a backtest). As a baseline check, include an MLP model’s results and maybe a persistence model (no-change or always predict small move) to quantify skill. Then, if the LSTM baseline shows promise (e.g., it’s able to pick up a decent number of the 3–5% moves with acceptable false positives), consider incrementally enhancing the architecture: adding a CNN layer (forming a CNN-LSTM) to see if short-term pattern extraction boosts accuracy, and/or adding an attention mechanism to help the LSTM focus on salient moments (forming an LSTM+Attention or using a TFT). If time permits, also compare with a small Transformer encoder – treat it as another competitor model in your experiments. The one with the best validation/backtest performance can be chosen for deployment. The rationale for prioritizing LSTM is its balance of performance and simplicity for this task. It has been widely used, so there are established best practices for tuning it on financial data (e.g., how to do sequence scaling, handling of time gaps like weekends, etc.). Additionally, many open-source examples and code are available for LSTMs on stock data, which can accelerate development. On the other hand, Transformers, while exciting, are newer in finance and might require more finicky tuning to get right (choice of learning rate schedule, etc.). Therefore, it’s practical to get a solid LSTM model running and measure its success, before diving into Transformers. In terms of expected outcomes: We anticipate the LSTM (or CNN-LSTM) model will outperform the baseline MLP by a good margin – consistent with literature where deep sequence models improved prediction error and trading metrics
techscience.com
onlinelibrary.wiley.com
. With the inclusion of the sentiment feature and multiple technical indicators, the model should be able to identify scenarios that precede a ~3–5% price move (for instance, a combination of bullish technical patterns and positive sentiment surge). The success will be measured not just by accuracy or MSE, but also by trading metrics like precision of signals (to avoid false alarms in a live trading context) and recall (catching a sizable fraction of true opportunities). Finally, a brief note on robustness: whichever model is chosen, it should be continually retrained or at least updated with new data as market conditions evolve
techscience.com
. Deep models can “drift” if the market enters a regime not represented in training data. So Phase 1 implementation should include a plan for model retraining (maybe weekly or monthly) using the latest data, and validation to ensure it’s not degrading. This will keep the model aligned with the current market, maintaining its edge in predicting those 3–5% moves. References (Literature Examples & Citations): The above recommendations and analyses are supported by extensive studies. Chen et al. (2024) provides a comprehensive review confirming the dominance of CNN-LSTM hybrids in recent financial forecasting research
techscience.com
techscience.com
. Empirical comparisons by Duong et al. (2022) illustrate how RNN-based models generally outperform MLPs and even show cases where CNNs or RNNs excel in different sectors
annals-csis.org
annals-csis.org
. Lin (2023) highlights the potential of Transformers, noting they capture unique dependencies and can be more promising in the long run
atlantis-press.com
. The importance of technical indicators is echoed by H. Lee et al. (2024), who demonstrated improved accuracy by integrating an ESG sentiment index with technicals
nature.com
. The effectiveness of certain indicators like KST and the benefit of ensembling models focusing on different indicators was shown by Saud & Shakya (2024)
oaji.net
. These sources, along with others cited throughout, form the knowledge base underpinning the recommended approach. In conclusion, Phase 1 should implement a supervised LSTM model (potentially enhanced with CNN preprocessing), using the rich set of technical indicators and sentiment as inputs. This model is expected to provide a strong baseline capability to predict mid-horizon trading signals, capturing the complex temporal patterns leading up to 3–5% return opportunities. As you evaluate its performance, you can then iterate – possibly incorporating attention mechanisms or trying out a Transformer – to further refine the predictive accuracy and reliability of the trading system.