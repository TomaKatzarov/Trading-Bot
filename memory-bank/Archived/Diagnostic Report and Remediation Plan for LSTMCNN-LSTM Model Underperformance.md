Diagnostic Report and Remediation Plan for LSTM/CNN-LSTM Model Underperformance
Diagnostic Analysis of Model Underperformance
The custom LSTM and CNN-LSTM models for the trading signal classification task have shown severe underperformance in both validation metrics and live backtesting. The symptoms are clear: extremely low F1 scores for the positive “BUY_SIGNAL” class (e.g. CNN-LSTM best Val F1 ≈ 0.03) and degenerate model behavior in backtests (e.g. outputting nearly constant high probabilities leading to zero executed trades). Below we analyze the root causes across model design, data, training procedure, and evaluation logic:
Model Architecture & Complexity Issues
Overly Complex or Inflexible Architecture: The CNN-LSTM hybrid model introduced additional convolution layers without yielding better feature extraction, as evidenced by its best validation F1 of only 0.0319 – far worse than the simpler LSTM. This suggests the CNN-LSTM may be over-parameterized relative to the dataset, causing underfitting or unstable training. In contrast, the LSTM (3 layers, 196 hidden units) achieved higher validation scores, but still struggled to generalize. The absence of any attention mechanism or memory beyond the fixed LSTM layers means the models might not be focusing on the most critical time steps or patterns. Traditional LSTMs can capture sequence information, but without attention they may dilute important signals over long sequences, especially in noisy financial data. The current LSTM architecture also lacks techniques like bidirectional context or explicit long-range memory, which further limits performance on multi-hour horizon predictions. Additionally, no batch normalization or layer normalization is used in the network stacks – this can lead to slower convergence and internal covariate shift, especially in deeper models. Overall, the architectures in use (plain LSTM, CNN-LSTM) are not effectively capturing the complex, sparse patterns needed for mid-horizon price moves, either due to insufficient model expressiveness (LSTM without attention) or over-complexity given limited data (CNN-LSTM overfitting noise). LSTM vs. CNN-LSTM Behavior: The LSTM model did achieve a higher positive-class F1 than the CNN-LSTM, but still very low (Test F1 ≈0.13). The CNN-LSTM’s convolutional layers did not help, likely because the receptive field of the conv layers was small (kernels of size 3 and 5) and no pooling was used, so the LSTM still had to model long sequences without significant dimensionality reduction. This complexity might have led to training difficulty (the Optuna search may not have found a good configuration in 50 trials) and potential overfitting on random fluctuations. In short, the architecture choices to combine CNN and LSTM did not yield benefits – possibly due to the small dataset and highly noisy nature of features, where a simpler sequence model (or one with attention) is more appropriate. The lack of gating alternatives like GRUs (which can be easier to train) also means we haven’t tried architectures that might converge faster on this data. Without architectural elements to emphasize salient events (e.g. sudden indicator moves or sentiment spikes), the models likely treat all time steps equally and get overwhelmed by noise.
Data Handling and Imbalance Problems
Severe Class Imbalance: The dataset’s positive class (buy signal) is extremely underrepresented (as is typical – profitable opportunities are rare). Initial training on individual symbols revealed this imbalance clearly and produced “unreliable” results. The team responded by combining data from multiple symbols to increase positive samples. While aggregation helped increase data volume, the positive class is still a small fraction, leading the models to favor the negative class. In fact, an easy high-accuracy strategy for the model is to predict the majority class almost always. Earlier runs showed the model often did exactly this – yielding high overall accuracy but almost never catching positive events (recall near 0). To combat this, class weighting and focal loss were introduced. However, these remedies led to the opposite extreme: the best LSTM with Focal Loss started outputting very high probabilities for the positive class on virtually all inputs. This explains the backtest behavior where the LSTM gave a ~0.99 probability constantly – by weighting the minority class so heavily, the model learned a trivial solution to maximize recall (predict “BUY” for everything, ensuring no positive is missed). This precision-recall trade-off imbalance is a direct consequence of the class weighting strategy. In summary, the dataset’s imbalance (exacerbated by a stringent +5% profit target) is a root cause of unstable model behavior: either ignoring positives completely or over-predicting positives depending on loss settings. The data sampling strategy has not yet found a middle ground. Aggregation and Data Context: Combining multiple stocks’ data (AAPL, MSFT, NVDA, etc.) was logical to alleviate imbalance. However, this introduces variation in feature distributions and potentially different market regimes across stocks. The current preprocessing uses a single global scaler and does not provide the model any identifier for which symbol a given sequence comes from. The model is forced to treat all sequences homogeneously. This could confuse the network – patterns effective for one stock might differ for another, and without a stock ID feature, the model cannot adjust its expectations. Moreover, if the train/validation/test split was not stratified by symbol and time, there is a risk of data leakage or distribution shift: e.g. the model might have seen later data from AAPL in training while trying to predict earlier AAPL in testing. (Ideally, splits should be done per-symbol chronologically, then combined, rather than random global splits.) If this wasn’t done, it could inflate validation performance artificially while still failing on true forward-chronology tests. The mention that validation metrics were “unreliable” for individual symbols hints that the splitting strategy needed careful design. It appears the current approach still lacks a robust cross-validation that respects time order. Finally, there may be missing data or misaligned features (e.g. sentiment scores align daily vs hourly bars) – any imputation errors or misalignment in the nn_data_preprocessor could add noise. Ensuring that each feature (9 technical indicators and 1 sentiment) is properly scaled and aligned per timestamp is critical; any inconsistency here would hurt the model’s ability to learn.
Training Procedure & Hyperparameter Factors
Suboptimal Training Regime: Training has so far relied on one-shot optimization (with early stopping) and Optuna hyperparameter searches for each architecture. The LSTM’s best result came from extensive tuning (learning rate 5e-4, dropout 0.2, focal alpha 0.99). The CNN-LSTM had a similar search but achieved a very poor optimum, suggesting the optimization might have gotten stuck in a bad local minimum (or the model just cannot fit the data well). It’s possible the learning rate schedule was not ideal – e.g. a fixed small LR (5e-5 in CNN-LSTM best trial is very low) might cause under-training, whereas too high would overshoot. There is no mention of using a learning rate scheduler or cyclic schedule, so training may not be fully exploring the loss landscape. Additionally, limited training epochs (with early stopping after a few epochs once validation stops improving) might cause underfitting in the CNN-LSTM, which could need more epochs to adjust its many parameters. The training might also not have used gradient clipping, which can be important for LSTMs to prevent exploding gradients given the sequence length – if not used, training instability might have been an issue (though no explicit mention of gradient problems in logs). Regularization and Overfitting: The LSTM model achieved a validation macro F1 of ~0.50, but its positive-class test F1 dropped to ~0.13. This gap suggests some overfitting or validation over-optimism. The team did use dropout (0.2) in the LSTM, which helps, but other regularization like weight decay (L2 regularization) was not mentioned. Without weight decay, the model’s weights could grow large to fit minority class examples, leading to over-confidence. The CNN-LSTM’s extremely low performance could indicate it was effectively not learning (possibly overfitting training noise or stuck in trivial predictions) – regularization like batch norm might have helped stabilize its convolution layers, but wasn’t in place. Also, no ensemble or averaging of multiple training runs was done – one model’s performance might be brittle. Finally, the hyperparameter tuning strategy may have been constrained: Optuna was run for 50 trials on CNN-LSTM, which might be insufficient given the high dimensional search space (many layer sizes, learning rates, etc.). It’s possible that better hyperparameters (or architectural tweaks) simply weren’t found in the limited search. In summary, the training process hasn’t fully squeezed out the models’ potential – improvements like dynamic LR schedules, more extensive searches (or different optimization techniques), and stronger regularization could yield better models.
Evaluation Metric & Backtesting Flaws
Metric Misalignment with Goals: The primary metric optimized was F1 score for the positive class, which is a reasonable proxy for catching rare events, but may not align perfectly with trading profitability. F1 treats false positives and false negatives as equally costly, but in trading, false positives (bad trades) often have a different cost profile than false negatives (missed opportunities). The optimal classification threshold was chosen to maximize validation F1. This led to thresholds around 0.66–0.70 for the best LSTM. However, maximizing F1 might still result in a strategy that trades too often with low precision. For instance, the best LSTM threshold of 0.6633 still yielded a model that signaled virtually every hour for AAPL (probabilities ~0.914 ≫ 0.66) and thus would attempt continuous trading. In backtesting, this manifested as the “zero trades” issue: the strategy likely entered a trade immediately and never exited because the model never dropped below threshold to give an exit signal. The evaluation process did not catch this because it focused on classification metrics rather than simulating trades during validation. Additionally, no cost/benefit analysis (like expected return per trade or Sharpe ratio) was used in model selection – a model could conceivably have a decent F1 but still lose money if it generates too many false signals. Backtesting Implementation Issues: The SupervisedNNStrategy backtester integration was initially buggy (loading the wrong model file led to outputs of ~0.9999 and no trades). Even after fixing the model file, the strategy logic still saw “0 trades” because the model output was persistently high. This indicates a logic issue: the strategy likely only counts a trade when a transition from below-threshold to above-threshold occurs. If the probability is always above threshold from the first timestep, the strategy might interpret that as never receiving a fresh buy signal (because it never crossed from false to true – it started true and stayed true). Essentially, the **strategy never enters because it expects a signal change. This is a flaw in how the model’s continuous predictions are translated into discrete trades. Moreover, backtesting on only one asset (AAPL) is not enough – the model should be evaluated on multiple symbols or a rolling time window. There’s no evidence of a walk-forward backtest (e.g. testing the model on data from a later period than it was trained on, across multiple windows). The current evaluation might therefore be too narrow. Finally, validation set instability (mentioned in logs) suggests that some splits had no positive examples or other issues, meaning the evaluation of model performance could be noisy or biased. This complicates reliable model development. Summary of Failure Points: In combination, these issues paint a clear picture: The models are not extracting the right signals (architecture issues, no attention), are hamstrung by data rarity (extreme class imbalance leading to pathological predictions), and the training/evaluation setup hasn’t yet found a balance where the model outputs meaningful, sparse “buy” signals. The evaluation loop didn’t initially catch the degenerate behavior until actual backtesting, indicating a need to align metrics with real trading outcomes. In short, architectural limitations, data imbalance, inadequate regularization, and misaligned evaluation criteria have all contributed to the poor performance of the LSTM and CNN-LSTM models.
Recommendations and Remediation Strategies
To address the above failure points, we propose a comprehensive set of solutions. These recommendations span model architecture changes, data and feature handling improvements, training procedure enhancements, and evaluation protocol fixes. The goal is to fundamentally improve the recall-precision balance of the model (so it finds more true positive signals without excessive false alarms) and ensure that evaluation aligns with actual trading success. All recommendations are made in light of the project’s constraints (local training, available data) and the eventual transition to Reinforcement Learning. Key suggestions include introducing an attention mechanism to the sequence model, using advanced regularization and optimization (like learning rate scheduling and weight decay), performing data augmentation and re-balancing, adopting an improved training curriculum, and revising how models are evaluated and selected. Specific actions are detailed below:
Model Architecture Enhancements
Integrate an Attention Mechanism: To allow the model to focus on the most relevant time steps and features, incorporate an attention layer into the LSTM architecture. For example, implement a self-attention or attention pooling after the LSTM layers. This would compute attention weights over the LSTM’s hidden states, enabling the model to weight critical movements (spikes in indicators or sentiment shifts) more heavily when producing the output. Attention mechanisms have proven effective in sequence tasks by capturing long-range dependencies and highlighting salient signals. In practice, we can add an Attention-LSTM hybrid: the LSTM processes the sequence, then an attention module (a small feed-forward network computing alignment scores) produces a weighted sum of hidden states for the final output layer. This should help address the “dilution” of important events in long input windows and improve the model’s signal-to-noise ratio on noisy financial sequences.
Evaluate GRU and Simplified Architectures: Replace or compare against LSTM with a GRU-based model. GRUs are simpler gated recurrent units that often train faster and generalize as well as LSTMs on many tasks. The project already has a GRUModel implemented; we should leverage it. A GRU model with similar capacity (e.g. 1-2 layers, ~100 units) might avoid some overfitting due to having fewer parameters and could be easier to tune. We should run hyperparameter tests for a GRU model and see if it achieves comparable or better F1. Additionally, reconsider the necessity of the CNN front-end. It might be beneficial to simplify the CNN-LSTM or drop it in favor of either: (a) an LSTM with attention (as above), or (b) a pure CNN or CNN+attention model. CNNs could still be useful to capture very short-term patterns (e.g. quick jumps), but they should be used with max pooling or dilated convolutions to extend their receptive field. One idea is a two-stream model: a CNN path for high-frequency local patterns and an LSTM/attention path for longer patterns, merged before the output. This is more experimental, so the immediate step is to get the attention-augmented LSTM/GRU working first. Overall, the architecture changes should aim to increase model expressiveness (via attention) while avoiding unnecessary complexity (use GRU or smaller LSTMs to prevent overfit).
Incorporate Batch Normalization / Layer Normalization: Add normalization layers to stabilize training. For instance, after the convolutional layers in the CNN-LSTM (if kept), insert BatchNorm1d to normalize the feature maps. This can reduce internal covariate shift and has a regularizing effect, allowing the CNN to learn smoother filters. For RNNs like LSTM/GRU, Layer Normalization is often more appropriate (normalizing across the hidden units of a time step) – it can be integrated inside the LSTM/GRU cells (PyTorch has nn.LSTM with layer norm option, or use nn.LayerNorm on the concatenated hidden states between layers). Normalization will help the model generalize by preventing activation distributions from drifting, and it can also speed up convergence. This is especially important given the diverse input scaling (technical indicators vs sentiment). By normalizing, we ensure the network can handle different feature magnitudes consistently.
Consider Transformer or Hybrid Models (Long-Term): If the above steps still fall short of capturing long-range dependencies, we should explore a small Transformer-based model as suggested in the strategic plan. A transformer encoder with a few self-attention layers could directly model temporal relationships without recurrence. However, transformers can easily overfit small datasets, so this should be approached cautiously and likely after trying the simpler attention-LSTM/GRU. A possible approach is a hybrid: use an LSTM to encode shorter sequences and a transformer encoder on top for capturing interactions over longer horizons (or vice versa). Another idea is to use Temporal Convolutional Networks (TCN) or Dilated CNNs as an alternative to RNNs, which have shown success in some time-series tasks. These advanced architectures would be Phase 2 experiments – if the immediate fixes don’t yield satisfactory results, we then implement and evaluate a transformer or TCN as part of a model redesign.
Model Input Representation: Modify the input representation if needed to assist the model. Currently, the input is a fixed window of length N (20 hours) of 10 features, flattened for MLP or in sequence for LSTM. We might enrich this by adding an asset indicator feature (one-hot encode the symbol or a learned embedding per symbol) appended to each input vector. This would explicitly tell the model which asset’s sequence it’s dealing with, allowing it to adjust its internal dynamics per asset. This could improve performance since certain patterns or indicator thresholds that trigger buys may differ by asset. The embedding could be small (e.g. 3-5 dimensions for each symbol) and would be constant across the sequence window. Including this in the input to the first layer of the network can help create a multi-asset model that is aware of asset-specific behavior. We should also ensure all features are scaled properly (with the global scaler or asset-specific normalization if needed) so that the model isn’t biased by scale differences. If global scaling is used, adding the asset ID feature will help the model internally rescale or adjust for each asset’s volatility.
Regularization and Optimization Improvements
Employ Learning Rate Scheduling: Introduce a learning rate scheduler to avoid premature convergence or getting stuck in minima. For example, use ReduceLROnPlateau on validation loss/F1 – if the metric doesn’t improve for a few epochs, reduce the LR by a factor (e.g. 0.1). This will let the model make finer updates as it approaches a plateau, potentially squeezing out extra performance. Another effective approach is the One-Cycle Policy or Cosine Annealing scheduler, which can encourage better generalization by varying the learning rate during training. By dynamically adjusting the learning rate, we might escape the flat regions of the loss landscape that the CNN-LSTM likely got stuck in. This is a straightforward change using PyTorch’s scheduler utilities and can be tuned on the fly.
Increase Epochs with Early Stopping Patience: Allow the models to train for more epochs now that we have more regularization in place. The previous best LSTM was saved at epoch 6, but CNN-LSTM’s training might have stopped early due to lack of improvement. We should extend the patience in early stopping (e.g. allow 10–15 epochs without improvement before stopping) especially for the new architectures. A longer training time combined with LR reduction can let the network slowly improve on the minority class patterns rather than stopping too soon. We must monitor for overfitting (validation metrics) and perhaps employ k-fold cross-validation (discussed later) to ensure more epochs are truly beneficial.
Apply Weight Decay (L2 Regularization): Enable a weight decay penalty on network parameters during optimizer setup. Even a small weight decay (e.g. 1e-5 or 1e-4) can prevent weights from growing too large and overfitting. It will inherently penalize extreme model outputs (since very high weights lead to high loss due to the penalty), which could counteract the tendency to output 0.99 probability for every input. This should help in moderating the model’s confidence. We would incorporate weight decay in the optimizer (Optuna can also tune the weight decay value). This is especially useful for the fully-connected and convolution layers. Empirically, weight decay often improves generalization in neural networks by keeping weights smaller, which is likely beneficial in our high-noise, low-signal environment.
Use Dropout and Regularization Strategically: The LSTM already uses dropout (0.2); we should maintain or even increase dropout in the new experiments (e.g. try 0.3–0.5 in some layers). We can also apply dropout to the input layer or between CNN and LSTM layers (if using CNN-LSTM) to further reduce overfitting. Another technique is spatial dropout for CNNs (dropping entire feature maps) which forces the CNN to not rely too heavily on any one filter. Ensure that for recurrent layers, we use recurrent dropout (dropout on connections between time steps, if supported) in addition to standard dropout on outputs – this can regularize LSTMs/GRUs effectively
file-che2a8wvrfblvtn6lkztwp
. However, with attention, we must be careful to also dropout attention weights (many implementations do this by default). We will experiment with dropout rates and possibly let Optuna tune the optimal value. With heavier regularization, the model will be less likely to memorize spurious patterns, hopefully improving generalization to new data.
Gradient Clipping: Enable gradient clipping (e.g. clip norms to a certain threshold like 5.0) during training to stabilize any occasional gradient explosions. This is a preventive measure: while we have not explicitly seen gradient explosion issues in logs, training an attention-enhanced RNN or deeper model could lead to large gradients. Clipping will ensure training doesn’t diverge and will also effectively limit how fast the model can shift towards predicting all positives in one update. It adds stability which could improve the final result by avoiding catastrophic iterations.
Optimize Threshold Post-Training: Instead of fixing the threshold purely by validation F1, consider treating the threshold as a parameter to optimize (for example, include it in a small grid search after training, optimizing a more trading-related metric). We could perform a brief search for the threshold that maximizes validation precision at a certain recall or even validation expected profit (if we simulate trades on the validation set). This isn’t a typical “regularization” of training per se, but it is an optimization of the model’s decision boundary that can drastically affect results. The “optimal” F1 threshold may be too low (causing too many trades). We may want to pick a higher threshold that yields, say, precision > 0.5 even if recall is lower. This can be done by examining the precision-recall curve (already available from metrics) and choosing a point that aligns with our risk tolerance. Such threshold tuning post-training will ensure the model’s outputs are used in the most effective way for the strategy.
Data Preprocessing and Augmentation Strategies
Augment and Oversample Positive Cases: To combat class imbalance beyond weighting, introduce data augmentation for the minority class. Financial time series data can be augmented in limited but meaningful ways. One approach is jittering or noise addition: for each detected “BUY_SIGNAL” sequence, slightly perturb the input features (e.g. add small Gaussian noise to indicator values, or slightly shift the start time by a few bars if the event still occurs) to create additional synthetic positive examples. We must be careful to keep the label valid – e.g. if shifting the window by 1 hour earlier still results in the price hitting +5% after, we can label that as positive too. Another augmentation could be mixing features: for instance, if two different positive events occurred, create a new positive sample by combining the technical indicators of one with the sentiment series of another (this is speculative, but could help diversity). We can also use bootstrapping: sample with replacement from the set of positive examples to oversample them during training (this can be done via a WeightedRandomSampler or by explicitly duplicating positives). The decision log already mentioned SMOTE (Synthetic Minority Over-sampling Technique) – applying SMOTE on time-series sequences is non-trivial, but we could apply SMOTE on the feature space of the sequences (for example, take the last timestep’s features of positive cases and SMOTE those to create new “pseudo-positive” feature vectors, though this ignores sequence dynamics). A simpler approach: focus on oversampling – ensure that in each training epoch, we present each positive case multiple times (maybe 5× or 10×) with slight variations, until the effective class ratio is closer to balanced. This, combined with focal loss (with a less extreme alpha, see below), will encourage the model to learn the subtle patterns that precede a 5% jump, rather than learn that “positive is rare so ignore it” or “positive is heavily weighted so always predict it.” Augmentation will increase the minority class signal content in training, ideally improving recall without forcing the model to distort its outputs so extremely.
Revisit Focal Loss / Class Weight Hyperparameters: The current best LSTM used Focal Loss with α=0.99 (almost all weight on positives) and γ=2.0. We should experiment with less extreme weighting – for instance α around 0.5–0.8. A slightly lower α will still emphasize positives but not to the point of making the model indifferent to precision. It may also help to reduce γ (which controls how much to down-weight easy examples); a high γ=4.58 was used in CNN-LSTM tuning, which might have overly down-weighted easy negatives and focused the model on the hardest samples (which might be noise). By lowering γ (e.g. 1.0 or 1.5), we make the loss a bit more balanced between easy and hard examples. In short, tune the loss function parameters (or even consider alternative losses like a weighted cross-entropy with a moderate weight, or a Area Under PR curve optimization via differentiable approximation). These adjustments, coupled with oversampling, may yield a model that predicts positive more conservatively. Essentially, we want to moderate the training signal: currently it’s “don’t miss any positives!” which yielded too many false alarms; we want to shift it to “catch more positives, but avoid blatant false alarms.” A balanced loss configuration can achieve that.
Improved Feature Engineering: Examine the current 9 technical indicators and 1 sentiment feature to identify any weaknesses. It’s important to ensure these features indeed provide predictive signal for the +5% move event. We may consider engineering additional features: for example, volatility features (the standard deviation of returns over the last N hours), trend strength indicators (ADX, or correlation of price with its moving average), or relative performance features (how the stock performed relative to the market index in that window). If certain technical indicators are not informative, we might drop them to reduce noise. Conversely, we might add features: e.g., include the price level or returns themselves in the input. If not already included, the model might benefit from knowing the recent return or momentum explicitly. Given the timeframe (1-hour bars, 20-hour window), an indicator like a 20-hour RSI might overlap with what the model can infer, but including it won’t hurt. We should also ensure the sentiment feature is utilized effectively – if the sentiment score is daily, maybe we can create an hourly sentiment proxy (forward-fill the daily sentiment across its day’s hours, so the model sees a constant sentiment value within that day). Additionally, if sentiment is often neutral (no news) and coded as 0, the model might be effectively ignoring it. We could introduce a binary feature “news_presence” to distinguish 0 meaning “truly neutral sentiment” vs “no sentiment data”. Feature engineering can also involve dimensionality reduction: if some of the 9 technicals are highly correlated or redundant, using a PCA or selecting the top features could help. However, given the small number of features, this is likely not as critical as making sure we have the right features. We might perform a quick feature importance analysis (per 1.4.5.4 in the plan) using the trained model’s SHAP values or permutation importance to see which features contribute to predictions. If some features are useless or noise, removing them could improve generalization. The literature suggests focusing on trend, momentum, volatility, volume, and sentiment features – ensure our feature set covers each of these categories robustly. For example, if we lack a volatility measure, that’s a gap to fill.
Per-Symbol Data Handling: Adjust how data from different symbols is handled. If we continue with a single model for all symbols, strongly consider the earlier recommendation of adding a symbol identifier feature. Alternatively, we could revert to training separate models per symbol or per cluster of similar symbols. Initially this was avoided due to severe class imbalance per symbol, but after all the new techniques (augmentation, etc.), it might be feasible. A compromise is multi-task learning: have one neural network with a shared base but multiple output heads, one for each symbol (each head learns to predict signals for its symbol). This way, the model can learn some shared representation of market dynamics, while allowing symbol-specific calibration in the output layer. However, this adds complexity and may be unnecessary if an asset ID input can achieve a similar effect. We will likely pursue the asset ID feature first, as it’s simpler. We should also ensure that the train/val/test split is truly time-based to mimic real deployment: e.g., train on data up to 2024, validate on a recent slice, and test on the most recent slice (or one particular symbol). A walk-forward split approach could be integrated into data preparation – for instance, reserve the last X months of each symbol as test, and do not shuffle those into training. This will make evaluation more realistic and prevent any inadvertent lookahead.
Data Cleaning and Imputation: Double-check the preprocessing for any missing or anomalous data points. If any feature has NaNs (e.g., an indicator might be undefined for the first few periods or sentiment missing on weekends), ensure we impute them. Typically forward-fill or using a neutral value is fine (the nn_data_preprocessor.py likely handled this, but worth verifying). If there are outlier values (maybe a bad data point where price jumps impossibly), consider capping or removing those to not confuse the model. The scaler choice (StandardScaler) assumes a roughly normal distribution; if our features like volume or sentiment are skewed, a RobustScaler (which uses medians) might perform better by not reacting to outliers. We should evaluate if any features are highly skewed; if so, transform them (e.g., take log of volume or 1+vol) or use robust scaling. Ensuring high-quality, consistent input data will make the model training more effective and reduce the chance of it learning noise.
Improved Training Regime and Hyperparameter Tuning
Curriculum Learning (Progressive Task Difficulty): Implement a curriculum learning approach to gradually teach the model how to detect easier patterns before the full difficult task. For instance, start by training a model to detect a smaller profit target or longer horizon (a slightly more frequent event). As a hypothetical example, label data for, say, +2% profit before -2% stop within N hours – an easier objective that will have more positives. Train the network on this for some epochs (or pre-train it fully to a reasonable performance). Then use those weights as initialization and switch the task to the actual +5% target. The intuition is that the model will first learn general price movement patterns and not be as clueless when looking for the rarer big moves. Another curriculum strategy is based on time horizon: first train the model to predict direction in the next 1 hour (which is almost 50/50 and easier), then 2 hours, up to N hours. This could help the recurrent model build up an understanding of short-term vs long-term cues. We need to be careful to avoid catastrophic forgetting (when switching tasks, perhaps slowly blend the tasks or continue to give some easier examples as hints). This approach leverages the idea that learning simpler sub-tasks can bootstrap the network for the ultimate task.
Transfer Learning and Pretraining: In addition to curriculum learning, consider pretraining the model on a related task or larger dataset. One idea is to use unsupervised pretraining: e.g., train an autoencoder on the historical price sequences to reconstruct them, or a forecasting model to predict the next value(s). This would force the LSTM/GRU to learn general representations of the time-series structure without focusing on the buy signal. After such pretraining (on abundant unlabeled data), we can fine-tune the model on the classification task with labels. Another idea is to pretrain on a broader financial dataset (if available, e.g., use more symbols or a longer history than we eventually trade) or even on a simulated dataset where events are more frequent (for instance, simulate price series where 5% jumps happen more often, train on those to give the model an intuition of jump dynamics, then fine-tune on real data). Furthermore, since the project initially explored an LLM-based approach, we might reuse some insights from there – however, given that pivot, it’s likely not directly helpful except perhaps using any sentiment analysis knowledge gained. In summary, any form of initialization that is not random could help – starting training from a model that already understands market dynamics a bit (even if not perfectly) can improve convergence on the rare-event classification. We can utilize the existing infrastructure (models/pretrained_components/ as noted in system design) to save and load these pretrainings.
Advanced Hyperparameter Tuning Techniques: Continue using Optuna for hyperparameter optimization, but augment it with more sophisticated or extensive search strategies. For example, we can implement Bayesian Optimization (Optuna’s TPE is Bayesian) with an increased number of trials (say 100-200 trials) now that we have new hyperparameters (attention-related dimensions, weight decay, etc.). We should also consider Population-Based Training (PBT) or genetic algorithms for hyperparameter tuning. PBT would train a population of models in parallel, periodically selecting the best and mutating their hyperparameters, effectively learning not just model weights but also hyperparam schedules. This could help find, say, an evolving learning rate schedule or a combination of dropout rates that works best. Given the project is running locally, we have to balance the computational load – PBT can be heavy. However, we might simulate a lightweight version: e.g., run 10 models in parallel with different hyperparams and every few epochs, drop the worst 2 and spawn new ones from the best 2 (with slight perturbs). This can sometimes find better solutions than static hyperparam searches, especially in irregular loss landscapes. Additionally, we might use cross-validation in tuning: instead of optimizing just validation F1 on one split, use a couple of different validation splits (or time folds) and aggregate the score. This makes the hyperparam selection more robust to overfitting on one validation set idiosyncrasies.
Experiment Tracking and Reproducibility: As we iterate, maintain rigorous tracking (likely already using MLflow per training script). It’s important to control randomness (set seeds for PyTorch and numpy) during comparisons so we can fairly judge improvements. Also consider performing multiple training runs for each configuration – sometimes one run might luck into a better or worse local minimum. For key experiments (like “LSTM+Attention with oversampling”), run 3 trials with different random seeds and average the results. This will give a more reliable estimate of performance and reduce the risk of selecting a model that was just lucky. Ensemble methods could also be an option: if two different runs yield slightly different models, an ensemble of them (e.g., average their predicted probabilities) might yield a more stable classifier. This could be used in final evaluation to modestly boost performance (at cost of complexity). It’s a secondary idea if single models remain unstable.
Gradual Model Refinement: Given that ultimately we plan to incorporate this model into an RL agent, keep the model size efficient. Training regimes should prefer a smaller well-regularized model over a gigantic one that only marginally improves F1. It may be better to have a model with F1 0.25 that is stable and quick, than one with F1 0.30 that is highly complex and slow (since the RL phase will require many forward passes). Therefore, as we tune, we might set constraints (like limit hidden units, etc.) and incorporate that into the hyperparam search objective (perhaps using a modest penalty for model size or inference time). This is more of a strategy to ensure our training focus aligns with deployment needs.
Evaluation Metrics and Validation Protocols
Adopt Precision-Recall and Profit-Based Metrics: Shift the primary evaluation metric from solely F1 (positive class) to a more holistic set that aligns with trading success. Specifically, monitor Precision, Recall, and PR-AUC for the positive class (the code already logs these). We should place particular emphasis on Precision at a target Recall or the Precision-Recall AUC, since in trading, a high precision (few false signals) can be more important than squeezing out the last bit of recall. For model selection, consider using PR-AUC as the optimization objective instead of F1. PR-AUC gives a sense of overall quality of the probability ranking, regardless of threshold. Moreover, incorporate a profit-based evaluation on the validation set: using the validation data (which the model doesn’t train on), simulate a simple backtest. For each predicted buy signal (with whatever threshold), calculate the actual return (if any) over the horizon, assume a nominal position size (or just count wins/losses). This can yield metrics like expected return per trade, win rate, average profit, max drawdown etc., on validation. While we can’t directly optimize the model on these (since that would require differentiating through the backtester), we can use them for model selection. For example, if Model A has slightly lower F1 but yields a 10% simulated profit on val, whereas Model B has higher F1 but yields 0% profit or negative (maybe due to more false trades), we should favor Model A. In practice, after each training, we run the model on validation, find its optimal threshold (maybe by max F1 or by max profit), then record the validation trading P&L metrics. The model that gives the best risk-adjusted return in validation (with some minimum precision constraint) should be chosen. This ensures our selected model is the one most likely to be profitable, not just the best at classifying.
Resolve Backtesting Signal Logic: Update the backtesting strategy implementation to handle continuous high-probability signals. One fix is to treat the first time step that the probability is above threshold as a BUY signal (enter trade), and then require a corresponding SELL signal or timeout to exit. Currently, it was expecting a drop below threshold to signal exit, which didn’t happen due to persistent high output. We can implement a maximum holding period equal to the prediction horizon N (e.g. 8 hours): if a trade is entered on a signal, automatically exit after N hours if no sell signal occurred (this realizes whatever profit/loss at that point). This way, even a model that predicts “always buy” will result in trades that last N hours and then close – we would see a flurry of trades and likely poor results, alerting us to the issue. Also, introduce an EXIT signal threshold if possible (maybe the model only predicts buy vs no-buy; but we could interpret a probability dropping below some lower threshold as an exit signal). Alternatively, one could use the model to predict both entry and exit in a multi-class setup (BUY vs HOLD vs SELL), but that complicates training. For now, implementing a time-based exit and counting only transitions from below to above threshold for entries will fix the “no trades counted” bug. We should test the strategy code with a scenario of constant high probabilities to ensure it enters a trade at t0 and either doesn’t double count or gets stuck. Logging in backtests should be enhanced to capture the sequence of model outputs and decisions – this will help debug any future issues.
Walk-Forward Validation: Strengthen the validation framework by using walk-forward splits. Instead of a single train-val-test split, perform multiple sequential evaluations. For example, train on data up to 2022, validate on 2023, test on 2024; then train up to 2023, test on 2024-2025, etc. This could be done in a rolling manner for each symbol or collectively. The idea is to measure how performance degrades or holds up as we move forward in time (market regimes change). It can also give more reliable estimates of performance variance. The progress plan’s Phase 3 includes rigorous testing with walk-forward and regime sensitivity – we should incorporate some of that earlier in a lightweight form. At least, for final model evaluation, do a chronological K-fold: e.g., split the entire historical period into 5 folds by time, each fold being a contiguous block, then perform 5 runs where each one uses 4 folds for training and the next fold for testing (ensuring training always uses data from before the test fold to respect causality). This will yield 5 test performances that can be averaged. Given our data volume might be limited, we can overlap folds slightly if needed. This approach will reduce the risk that we happened to get lucky or unlucky on a single test period. If the model shows consistently low positive-class recall or precision across all folds, we know it’s a fundamental issue; if some folds (time periods) it does well, that might indicate regime-dependent performance we can analyze (perhaps incorporate regime features or do adaptive training).
Stratified Sampling for Validation: When selecting validation sets for hyperparameter tuning, ensure they are stratified by class – i.e. contain a representative proportion of positive examples. Because positives are rare, a purely random time split might end up with a validation set that has zero or only a couple of positive instances, making metrics like F1 unstable. Stratification in time series is tricky because we can’t randomly shuffle, but we can choose a validation period that we know contains at least some events (for example, ensure the validation period includes a volatile market phase likely to have some +5% moves). In multi-symbol data, also stratify by symbol (so val isn’t, say, all one symbol). A possible approach: reserve a small piece of each symbol’s data for validation to get a mix. However, the primary guard against metric instability will be using cross-validation and multiple metrics as described. We should also calculate confidence intervals for metrics where possible (e.g. Wilson interval for precision/recall) to understand the uncertainty due to low sample count.
Feature Importance and Error Analysis: As part of evaluation, perform an in-depth error analysis on the improved model. Look at a confusion matrix to see how many false positives vs false negatives. Examine a few false positive cases manually – were those situations where technical indicators looked promising but price failed to hit the target? If so, is there an additional feature that could have disambiguated it (maybe an outside factor like news which our sentiment didn’t catch)? Similarly, examine false negatives – cases where a 5% rise happened but model missed it. Identify if there were common patterns in those that the model isn’t picking up (maybe a certain pattern of volume spike that we don’t have features to capture). This qualitative analysis will guide further feature engineering or threshold adjustments. Incorporating a feature importance analysis (1.4.5.4) will quantitatively tell us which features the model relied on; if some seem counterintuitive or too dominant, we might need to adjust (for example, if the model over-relies on sentiment and produces false positives whenever sentiment is slightly positive, maybe we need to refine the sentiment input).
Set Performance Criteria for Deployment Transition: Define clear criteria for what constitutes a “satisfactory” model ready to move on to RL integration. For instance, we might decide that we need Validation (or cross-validated) Positive-Class F1 > 0.25 and Precision > 0.25 (just as example numbers), or that the model in backtest yields a profit factor > 1.5 (total gains 1.5× total losses) and a reasonable number of trades. Having these targets will keep us oriented and prevent endless tweaking. As soon as a model meets the criteria, we can consider the supervised baseline “good enough” to proceed to the next phase (using RL to further improve it). These criteria should be documented and used in decision-making (for example, the decision to pivot to RL should reference that the baseline hit the required performance). This ensures that evaluation is not just retrospective but drives the project forward.
Phased Remediation Roadmap
To implement the above recommendations in an organized way, we propose a phased remediation plan. Each phase focuses on a set of improvements, building towards a robust model and eventually integrating with the RL agent. Below is the prioritized roadmap with estimated timelines:
Phase 1 (Immediate Fixes – Weeks 1-2): Supervised Model Stabilization
Objective: Quickly address the most critical issues to get a working baseline that makes some profitable trades.
Tasks:
Fix backtesting logic and Threshold: Update the SupervisedNNStrategy to handle continuous signals (enter at first signal, enforce exit by horizon). Set a conservative probability threshold initially (e.g. 0.9) to force fewer trades and observe performance.
Loss Function Tweak: Retrain the current best LSTM with a lower focal loss α (e.g. 0.9 instead of 0.99) to reduce the always-positive bias. Possibly try standard weighted cross-entropy as a comparison.
Quick Architecture Test: Run a quick experiment training the simpler GRU model (with current settings) to see if it already outperforms the LSTM/CNN-LSTM in F1 or stability. This will indicate if GRU should be part of the longer-term solution.
MLP Baseline Check: Complete the MLP Optuna study (Task 1.4.4.G) and evaluate the MLP’s performance. If the MLP yields similar low F1 (likely ~0.03-0.05), it confirms that the issue is not just the sequential models but the data/labels – reinforcing the need for data-centric fixes. If MLP surprisingly does better, that insight will be used (perhaps the time ordering is adding noise).
Expected Outcome: By the end of Phase 1, we expect to resolve the “zero trades” issue and have the model making a few trades in backtest. The positive-class precision and recall should both improve modestly (e.g. F1 from ~0.13 to ~0.2 on test) as we moderate the loss weighting. We also expect clarity on whether GRU offers an advantage. These immediate fixes set the stage for more extensive improvements. (Estimated timeline: within 2 weeks, i.e., by early June 2025.)
Phase 2 (Model Redesign & Data/Training Overhaul – Weeks 3-8): Comprehensive Improvements
Objective: Implement the major changes to architecture and training to significantly boost model performance.
Tasks:
Integrate Attention and Re-train: Develop an LSTM+Attention model (or GRU+Attention depending on Phase 1 results) and train it on the full combined dataset with class imbalance countermeasures. Use Optuna to tune new hyperparameters (e.g. attention dimension, context vector size). This includes updating core/models/nn_architectures.py to add an Attention layer class.
Augment Data & Oversample: Generate augmented positive samples and set up the training loader to oversample minority class sequences. Possibly implement a custom ImbalancedDatasetSampler or use PyTorch’s WeightedRandomSampler with computed weights. Run training with and without augmentation to measure impact.
Hyperparameter Search (Extended): Conduct a more exhaustive hyperparam search with the new architecture. This could be a two-stage process: first a broad Optuna search (100 trials) to find a rough good configuration, then a focused grid search around the best to fine-tune. Include parameters like dropout rate, weight decay, focal loss α/γ, learning rate schedule, etc. Also tune the classification threshold if possible by evaluating PR curves during training.
Regularization Techniques: Enable weight decay in the optimizer and try runs with different values. Add batch normalization layers as planned and verify training loss is smoother. Use gradient clipping in training (monitor if any gradients were actually clipped as an indicator of instability).
Feature Engineering: If not already done, incorporate the asset ID feature into the model input and confirm the model can differentiate assets. Add any new technical indicators found useful. Possibly create a “meta-feature” like a risk index (combining volatility and sentiment) for the model to use.
Cross-Validation Evaluation: As the model is being trained/tuned, perform a k-fold time-split evaluation (e.g., train on 2017-2023, test on 2024, etc., rotating) to ensure the improvements hold across time. Use these evaluations in choosing the final model configuration.
Monitoring Overfitting: During Phase 2, use MLflow or logs to keep an eye on training vs validation performance. We expect validation metrics to improve significantly if our changes work. If training metrics go much higher than validation (sign of overfit), increase dropout or simplify architecture accordingly.
Expected Outcome: By the end of Phase 2, we aim to have a substantially improved model ready for baseline evaluation. Concretely, the target is to achieve at least 15-20% precision and recall on the positive class (so F1 around 0.20–0.30) on out-of-sample data, and to see profitable backtest results (e.g., a positive net profit, reasonable trade count) on one or more assets. The model should be demonstrably better than random or trivial predictors. This phase should also produce a clear record of which techniques contributed most (for example, we might find that attention + moderate focal loss was the winning combo, whereas heavy augmentation might or might not help – we will document these). Phase 2 is expected to last about 4-6 weeks (June through mid-July 2025) given the experimentation involved.
Phase 3 (Hybrid Model Evaluation & Preparation for Deployment – Weeks 9-16): Transition to Reinforcement Learning
Objective: Take the validated supervised model and integrate it into a reinforcement learning framework, testing a hybrid trading strategy and moving towards live deployment.
Tasks:
Define Performance Threshold: First, confirm that the Phase 2 model meets the criteria to proceed (as defined in evaluation recommendations). Assuming it does, formally “freeze” this model as the baseline. If it falls just short, we might do one more iteration of tweaks, but let’s presume success.
Develop RL Environment: Begin work on the high-fidelity market simulator (if not already started in parallel). Use historical data to simulate trading with realistic constraints (transaction costs, slippage, etc.). Leverage the existing backtester but extend it for multi-episode training capability and more detailed reward calculations. This addresses Task 2.1 in the plan. Time estimate: ~2 weeks for a basic working environment since a lot of backtesting code exists.
Integrate the NN Model into an RL Agent: Design the RL agent (likely an Actor-Critic or DQN) that uses our neural network architecture as the policy and/or value function. For example, we can use the LSTM+Attention network as the policy network outputting action probabilities (buy, hold, sell). This corresponds to Task 2.2. We will initialize this policy network with the supervised model’s weights (transfer learning into the RL context). This gives the agent a strong starting point (it already knows how to predict good opportunities to some extent).
Define RL specifics: Set up the action space (probably discrete: Buy, Sell, Hold), state representation (we can use the same input features the NN uses, possibly augmented with position status), and reward function. The reward can be P&L-based (e.g. percentage return of each trade, or a shaped reward like +1 for profit, -1 for loss, etc., with perhaps a small penalty for holding to encourage shorter trades if desired). This is Task 2.3 – will need careful thought to align with our profit goals and risk management (e.g., incorporate drawdown penalties or Sharpe ratio components to reward).
Train the RL Agent: Run the RL training (Task 2.4) in the simulation environment. This will likely involve many episodes/epochs of training. We should monitor if the agent improves upon the baseline. One approach is Evaluation against baseline: periodically evaluate the policy in backtest and compare its performance to just using the fixed supervised model’s signals. The expectation is the RL agent can learn when to take signals or risk based on context better than a fixed threshold model. For instance, it might learn to skip trades during choppy sideways markets (even if the model says “buy”) because those yield poor reward, something a pure classifier can’t do. We may try different RL algorithms (start with something stable like DDPG or PPO if using continuous action for position sizing, or DQN if discrete actions suffice). The training might take a few weeks of experimenting, given the complexity and need for hyperparameter tuning in RL (exploration rate, etc.).
Hybrid Evaluation: Once the RL agent training is complete, evaluate it on out-of-sample data (simulated unseen market data or the most recent period). Compare the hybrid strategy’s performance (which now includes the learned policy for entries/exits) to the supervised baseline strategy. We hope to see improvements such as higher profit, lower drawdowns, or more stable performance across different market conditions. If the RL agent doesn’t yet outperform, we may iterate (tweak reward or model structure) or consider an ensemble (maybe use the supervised model’s signals and RL’s decisions together).
Final Paper Trading Prep: With a working RL-enhanced agent, prepare for deployment. This includes setting up the agent to work with live/paper trading data streams, ensuring we can feed real-time features into the model and get actions out. Also, implement any necessary safety checks (e.g., don’t trade if model confidence is low, etc.). Essentially, get everything ready for Phase 4 deployment in a paper trading environment.
Expected Outcome: By the end of Phase 3 (roughly ~2 months, mid-July to mid-September 2025), we aim to have a hybrid trading agent that uses the improved neural network and an RL policy to make trading decisions. This agent should be tested and show better performance than the original supervised strategy. We also expect to have all documentation updated (the decision log, progress reports) reflecting these changes. At this point, the project will be ready to move to Phase 4: Paper Trading Deployment where the agent is run on live market data in a risk-free environment to validate performance in real-time. Phase 3 is essentially the bridge between a good static model and a dynamic, adaptive trading agent. We will also finalize a “Baseline Performance Report” documenting the journey: initial model vs new model vs RL agent results, to justify the improvements before deploying.
The roadmap above is aggressive but achievable with focused effort. Phase 1 addresses quick wins, Phase 2 does the heavy lifting to fix the model, and Phase 3 leverages the improved model to start realizing the original vision of an autonomous trading agent that can learn and adapt (via RL). This phased approach ensures we stabilize the foundation first (a reliable supervised model) before adding complexity with RL, thereby increasing the likelihood of overall success. The plan aligns with the project goals by ultimately aiming to deliver a profitable trading agent and by using the supervised model improvements as a springboard into reinforcement learning, where further performance gains and adaptability can be achieved. Finally, throughout these phases, we will continuously monitor progress against the project’s KPIs (profitability, risk metrics, etc.) and adjust as needed. If certain steps fail to produce improvements, we will revisit the assumptions (for example, if attention doesn’t help, maybe a different architecture like a small transformer or an ensemble of models might be tried). The plan is iterative and evidence-based, meaning each recommendation will be evaluated rigorously and only adopted if it demonstrably contributes to the end goal of a reliable, profitable trading strategy.